{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine\n",
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import feather\n",
    "import os\n",
    "import torch\n",
    "import h5py\n",
    "import pickle\n",
    "\n",
    "\n",
    "# connect to db\n",
    "user = 'root'\n",
    "pswd = 'Curry5566'\n",
    "host = '127.0.0.1'\n",
    "port = '3306'\n",
    "db = 'transport'\n",
    "engine = create_engine(f\"mysql+pymysql://{user}:{pswd}@{host}:{port}/{db}?charset=utf8\")\n",
    "\n",
    "\n",
    "def getEndDate(startDate: str, days: int) -> str:\n",
    "    startDate += ' 00:00:00'\n",
    "    end = str((datetime.strptime(startDate, '%Y-%m-%d %H:%M:%S') + timedelta(days=days)).replace(microsecond=0))\n",
    "    return end\n",
    "\n",
    "def getRollingMean(startDate: str, endDate: str) -> pd.DataFrame:\n",
    "    \"\"\" Get the rolling mean from db\n",
    "        ```text\n",
    "        ---\n",
    "        @Params\n",
    "        startDate: The date for start, format='%Y-%m-%d'\n",
    "        endDate: The date for end, format='%Y-%m-%d'\n",
    "\n",
    "        ---\n",
    "        @Returns\n",
    "        DataFrame\n",
    "        ```\n",
    "    \"\"\"\n",
    "    sql  = \" SELECT \"\n",
    "    sql += \" \tSTAC.VDID, STAC.RoadName, STAC.`Start`, STAC.`End`, \"\n",
    "    sql += \" \tSTAC.RoadDirection, DYMC.Speed, DYMC.Occupancy, DYMC.Volume, \"\n",
    "    sql += \" \tSTAC.LocationMile, DYMC.DataCollectTime \"\n",
    "    sql += \" FROM ( \"\n",
    "    sql += \" \tSELECT \"\n",
    "    sql += \" \t\tVDSTC.id, VDSTC.VDID, ROAD.RoadName, SEC.`Start`, SEC.`End`, \"\n",
    "    sql += \" \t\tVDSTC.RoadDirection, VDSTC.LocationMile \"\n",
    "    sql += \" \tFROM vd_static_n5 VDSTC \"\n",
    "    sql += \" \tJOIN road_info ROAD ON VDSTC.RoadInfoID = ROAD.id \"\n",
    "    sql += \" \tJOIN section_info SEC ON ROAD.id = SEC.RoadInfoID \"\n",
    "    sql += \" \tAND VDSTC.LocationMile >= SEC.StartKM \"\n",
    "    sql += \" \tAND VDSTC.LocationMile <= SEC.EndKM \"\n",
    "    sql += \" \tWHERE VDSTC.Mainlane = 1 \"\n",
    "    sql += \" ) STAC JOIN ( \"\n",
    "    sql += \" \tSELECT \"\n",
    "    sql += \" \t\tVdStaticID, \"\n",
    "    sql += \" \t\tCASE \"\n",
    "    sql += \" \t\t\tWHEN MIN(Speed) = -99 THEN -99 \"\n",
    "    sql += \" \t\t\tELSE AVG(Speed) \"\n",
    "    sql += \" \t\tEND AS Speed,  \"\n",
    "    sql += \" \t\tCASE \"\n",
    "    sql += \" \t\t\tWHEN MIN(Occupancy) = -99 THEN -99 \"\n",
    "    sql += \" \t\t\tELSE AVG(Occupancy) \"\n",
    "    sql += \" \t\tEND AS Occupancy,  \"\n",
    "    sql += \" \t\tCASE \"\n",
    "    sql += \" \t\t\tWHEN MIN(Volume) = -99 THEN -99 \"\n",
    "    sql += \" \t\t\tELSE AVG(Volume) \"\n",
    "    sql += \" \t\tEND AS Volume, \"\n",
    "    sql += \" \t\tMAX(DataCollectTime) AS DataCollectTime, \"\n",
    "    sql += \" \t\t(UNIX_TIMESTAMP(DataCollectTime)-UNIX_TIMESTAMP(%(start)s)) DIV 300 \"\n",
    "    sql += \" \tFROM vd_dynamic_detail_n5_202301 \"\n",
    "    sql += \" \tWHERE id BETWEEN ( \"\n",
    "    sql += \" \t\tSELECT id FROM vd_dynamic_detail_n5_202301 \"\n",
    "    sql += \" \t\tWHERE DataCollectTime = %(start)s \"\n",
    "    sql += \" \t\tORDER BY id LIMIT 1 \"\n",
    "    sql += \" \t) AND ( \"\n",
    "    sql += \" \t\tSELECT id FROM vd_dynamic_detail_n5_202301 \"\n",
    "    sql += \" \t\tWHERE DataCollectTime < %(end)s \"\n",
    "    sql += \" \t\tORDER BY id DESC LIMIT 1 \"\n",
    "    sql += \" \t) \"\n",
    "    sql += \" \tGROUP BY VdStaticID, (UNIX_TIMESTAMP(DataCollectTime)-UNIX_TIMESTAMP(%(start)s)) DIV 300 \"\n",
    "    sql += \" ) DYMC ON STAC.id = DYMC.VdStaticID \"\n",
    "    sql += \" ORDER BY STAC.RoadDirection, STAC.LocationMile, DYMC.DataCollectTime; \"\n",
    "\n",
    "    df = pd.read_sql(sql, con=engine, params={'start': startDate, 'end': endDate})\n",
    "    engine.dispose()\n",
    "    return df.sort_values(by=['RoadDirection','DataCollectTime','LocationMile']).reset_index(drop=True)\n",
    "\n",
    "def getRollingMeanDaily(selectDate: str) -> pd.DataFrame:\n",
    "    sql  = \" SELECT \"\n",
    "    sql += \" \tSTAC.VDID, STAC.RoadName, STAC.`Start`, STAC.`End`, \"\n",
    "    sql += \" \tSTAC.RoadDirection, DYMC.Speed, DYMC.Occupancy, DYMC.Volume, \"\n",
    "    sql += \" \tSTAC.ActualLaneNum, STAC.LocationMile, STAC.isTunnel, DYMC.DataCollectTime \"\n",
    "    sql += \" FROM ( \"\n",
    "    sql += \" \tSELECT \"\n",
    "    sql += \" \t\tVDSTC.id, VDSTC.VDID, ROAD.RoadName, SEC.`Start`, SEC.`End`, \"\n",
    "    sql += \" \t\tVDSTC.ActualLaneNum, VDSTC.RoadDirection, VDSTC.LocationMile, \"\n",
    "    sql += \"        CASE \"\n",
    "    sql += \" \t        WHEN VDSTC.RoadDirection = 'S' AND VDSTC.LocationMile BETWEEN 15.203 AND 28.128 THEN 1 \"\n",
    "    sql += \" \t        WHEN VDSTC.RoadDirection = 'N' AND VDSTC.LocationMile BETWEEN 15.179 AND 28.134 THEN 1 \"\n",
    "    sql += \" \t        ELSE 0 \"\n",
    "    sql += \"        END AS isTunnel \"\n",
    "    sql += \" \tFROM fwy_n5.vd_static_2023 VDSTC \"\n",
    "    sql += \" \tJOIN transport.road_info ROAD ON VDSTC.RoadInfoID = ROAD.id \"\n",
    "    sql += \" \tJOIN transport.section_info SEC ON ROAD.id = SEC.RoadInfoID \"\n",
    "    sql += \" \tAND VDSTC.LocationMile >= SEC.StartKM \"\n",
    "    sql += \" \tAND VDSTC.LocationMile <= SEC.EndKM \"\n",
    "    sql += \" \tWHERE VDSTC.Mainlane = 1 \"\n",
    "    sql += \" ) STAC JOIN ( \"\n",
    "    sql += \" \tSELECT \"\n",
    "    sql += \" \t\tVdStaticID, \"\n",
    "    sql += \" \t\tCASE \"\n",
    "    sql += \" \t\t\tWHEN MIN(Speed) = -99 THEN -99 \"\n",
    "    sql += \" \t\t\tELSE AVG(Speed) \"\n",
    "    sql += \" \t\tEND AS Speed,  \"\n",
    "    sql += \" \t\tCASE \"\n",
    "    sql += \" \t\t\tWHEN MIN(Occupancy) = -99 THEN -99 \"\n",
    "    sql += \" \t\t\tELSE AVG(Occupancy) \"\n",
    "    sql += \" \t\tEND AS Occupancy,  \"\n",
    "    sql += \" \t\tCASE \"\n",
    "    sql += \" \t\t\tWHEN MIN(Volume) = -99 THEN -99 \"\n",
    "    sql += \" \t\t\tELSE AVG(Volume) \"\n",
    "    sql += \" \t\tEND AS Volume, \"\n",
    "    sql += \" \t\tMAX(DataCollectTime) AS DataCollectTime, \"\n",
    "    sql += \" \t\t(UNIX_TIMESTAMP(DataCollectTime)-UNIX_TIMESTAMP(%(selectDate)s)) DIV 300 \"\n",
    "    sql += \" \tFROM fwy_n5.vd_dynamic_detail_{} \".format(selectDate.replace('-',''))\n",
    "    sql += \" \tGROUP BY VdStaticID, (UNIX_TIMESTAMP(DataCollectTime)-UNIX_TIMESTAMP(%(selectDate)s)) DIV 300 \"\n",
    "    sql += \" ) DYMC ON STAC.id = DYMC.VdStaticID \"\n",
    "    sql += \" ORDER BY STAC.RoadDirection, STAC.LocationMile, DYMC.DataCollectTime; \"\n",
    "\n",
    "    df = pd.read_sql(sql, con=engine, params={'selectDate': selectDate})\n",
    "    engine.dispose()\n",
    "    return df.sort_values(by=['RoadDirection','DataCollectTime','LocationMile']).reset_index(drop=True)\n",
    "\n",
    "def groupVDs(df: pd.DataFrame, each: int) -> dict:\n",
    "    \"\"\" Get the dict of VD groups\n",
    "        ```text\n",
    "        ---\n",
    "        @Params\n",
    "        df: DataFrame which is referenced by.\n",
    "        each: The quantity of VDs would be considered as a group.\n",
    "\n",
    "        ---\n",
    "        @Returns\n",
    "        vdGroups: The keys are the VDs we focus on, and the values are the collections of VDs which are correlated corresponding to the keys.\n",
    "        ```\n",
    "    \"\"\"\n",
    "    vdGroups = {}\n",
    "    lb = each // 2\n",
    "    ub = each - (each // 2)\n",
    "    for vdid in df['VDID'].unique():\n",
    "        vdGroups.setdefault(f\"{vdid}\", [])\n",
    "    for no, vdid in enumerate(df['VDID'].unique()):\n",
    "        startIdx = max(no-lb, 0)\n",
    "        endIdx = min(no+ub, len(df['VDID'].unique())-1)\n",
    "        vdGroups[f\"{vdid}\"] += list(df['VDID'].unique()[startIdx:no]) + list(df['VDID'].unique()[no:endIdx])\n",
    "\n",
    "    delList = []\n",
    "    for k in vdGroups.keys():\n",
    "        if (len(vdGroups[k]) != each):\n",
    "            delList.append(k)\n",
    "    for k in delList:\n",
    "        del vdGroups[k]\n",
    "    \n",
    "    return vdGroups\n",
    "\n",
    "def genSamples(df: pd.DataFrame, vdGroups: dict, groupKey: str, each: int, timeWindow: int = 30) -> tuple:\n",
    "    \"\"\" Generate samples for each traffic data (speed, volume, and occupancy)\n",
    "        ```text\n",
    "        ---\n",
    "        @Params\n",
    "        df: \n",
    "        vdGroups: The outpur of groupVDs(),\n",
    "        groupKey: The key of vdGroups,\n",
    "        each: The quantity of VDs would be considered as a group,\n",
    "        timeWindow: The length of period we consider, and the default value is 30 (minutes).\n",
    "\n",
    "        ---\n",
    "        @Returns\n",
    "        speeds: list with each item as a tuple, all of them are represented (X,y).\n",
    "        vols: list with each item as a tuple, all of them are represented (X,y).\n",
    "        occs: list with each item as a tuple, all of them are represented (X,y).\n",
    "        ```\n",
    "    \"\"\"\n",
    "    speeds, vols, occs = [], [], []\n",
    "    tmpDf = df.loc[(df['VDID'].isin(vdGroups[f\"{groupKey}\"]))].sort_values(by=['LocationMile', 'DataCollectTime'])\n",
    "\n",
    "    indices = [x for x in range(0, tmpDf.shape[0]+1, tmpDf.shape[0]//each)]\n",
    "    speedMatx = np.zeros((each, tmpDf.shape[0]//each))\n",
    "    volMatx = np.zeros((each, tmpDf.shape[0]//each))\n",
    "    occMatx = np.zeros((each, tmpDf.shape[0]//each))\n",
    "    for i, j, k in zip(range(each), indices[:-1], indices[1:]):\n",
    "        speedMatx[i] += tmpDf.iloc[j:k,:]['Speed'].to_numpy()\n",
    "        volMatx[i] += tmpDf.iloc[j:k,:]['Volume'].to_numpy()\n",
    "        occMatx[i] += tmpDf.iloc[j:k,:]['Occupancy'].to_numpy()\n",
    "\n",
    "    sliceLen = int((timeWindow / 5) + 1)\n",
    "    for x in range(speedMatx.shape[1]//sliceLen*sliceLen-(sliceLen-1)):\n",
    "        speeds.append((speedMatx[:,x:x+sliceLen][:,:-1], speedMatx[:,x:x+sliceLen][:,[-1]]))\n",
    "        vols.append((volMatx[:,x:x+sliceLen][:,:-1], volMatx[:,x:x+sliceLen][:,[-1]]))\n",
    "        occs.append((occMatx[:,x:x+sliceLen][:,:-1], occMatx[:,x:x+sliceLen][:,[-1]]))\n",
    "    \n",
    "    return speeds, vols, occs\n",
    "\n",
    "def genTensors(speeds: list, vols: list) -> list:\n",
    "    \"\"\" Generate torch.Tensors.\n",
    "        The sizes of the tensors are `[batch, 2, each, 6]`, and `each` depends on how many VDs regarded as a group.\n",
    "    \"\"\"\n",
    "    dataCollection = []\n",
    "    for s, v in zip(speeds, vols):\n",
    "        s = torch.tensor(s, dtype=torch.float).unsqueeze(0).unsqueeze(0)\n",
    "        v = torch.tensor(v, dtype=torch.float).unsqueeze(0).unsqueeze(0)\n",
    "        dataCollection.append(torch.concat([s, v], dim=1))\n",
    "    return dataCollection\n",
    "\n",
    "def train_test_split(speedCollection, volCollection, train_size=None, test_size=None, random_number=42):\n",
    "    np.random.seed(random_number)\n",
    "    if train_size:\n",
    "        trainDataIdx = np.random.choice(\n",
    "            len(speedCollection),\n",
    "            int(train_size * len(speedCollection)),\n",
    "            replace=False\n",
    "        )\n",
    "        testDataIdx = set([i for i in range(len(speedCollection))]) -\\\n",
    "                      set(trainDataIdx)\n",
    "    \n",
    "    elif test_size:\n",
    "        testDataIdx = np.random.choice(\n",
    "            len(speedCollection),\n",
    "            int(test_size * len(speedCollection)),\n",
    "            replace=False\n",
    "        )\n",
    "        trainDataIdx = set([i for i in range(len(speedCollection))]) -\\\n",
    "                       set(testDataIdx)\n",
    "        \n",
    "    trainSpeed = list(pd.Series(speedCollection)[list(trainDataIdx)])\n",
    "    trainVol = list(pd.Series(volCollection)[list(trainDataIdx)])\n",
    "    testSpeed = list(pd.Series(speedCollection)[list(testDataIdx)])\n",
    "    testVol = list(pd.Series(volCollection)[list(testDataIdx)])\n",
    "\n",
    "    return trainSpeed, trainVol, testSpeed, testVol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 取得一年份資料\n",
    "# firstDate = list(map(lambda x: datetime.strftime(x, '%Y-%m-%d'), list(pd.date_range('2023-01-01', '2023-12-31', freq='MS'))))\n",
    "# lastDate = list(map(lambda x: datetime.strftime(x, '%Y-%m-%d'), list(pd.date_range('2023-01-01', '2023-12-31', freq='ME'))))\n",
    "# for first, last in zip(firstDate, lastDate):\n",
    "#     dataframes = []\n",
    "#     dateList = list(map(lambda x: datetime.strftime(x, '%Y-%m-%d'), list(pd.date_range(first, last))))\n",
    "#     for date in dateList:\n",
    "#         print(date)\n",
    "#         dataframes.append(getRollingMeanDaily(date))\n",
    "#     dataframes = pd.concat(dataframes).reset_index(drop=True)\n",
    "#     display(dataframes)\n",
    "#     feather.write_dataframe(dataframes, dest=f\"./nfb2023/{date[:7].replace('-','')}.feather\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "monthlyStarts = list(map(lambda x: datetime.strftime(x, '%Y-%m-%d'), list(pd.date_range('2023-02-01', '2023-02-28', freq='MS'))))\n",
    "monthlyEnds = list(map(lambda x: datetime.strftime(x, '%Y-%m-%d'), list(pd.date_range('2023-02-01', '2023-02-28', freq='ME'))))\n",
    "\n",
    "for start, end in zip(monthlyStarts, monthlyEnds):\n",
    "    dataframes = []\n",
    "    dateList = list(map(lambda x: datetime.strftime(x, '%Y-%m-%d'), list(pd.date_range(start, end))))\n",
    "    print(start[:7].replace('-',''))\n",
    "    for date in dateList:\n",
    "        print(date)\n",
    "        dataframes.append(getRollingMeanDaily(date))\n",
    "    dataframes = pd.concat(dataframes).reset_index(drop=True)\n",
    "    # display(dataframes)\n",
    "    feather.write_dataframe(dataframes, dest=f\"./nfb2023/{start[:7].replace('-','')}.feather\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.listdir('./nfb2023')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>VDID</th>\n",
       "      <th>RoadName</th>\n",
       "      <th>Start</th>\n",
       "      <th>End</th>\n",
       "      <th>RoadDirection</th>\n",
       "      <th>Speed</th>\n",
       "      <th>Occupancy</th>\n",
       "      <th>Volume</th>\n",
       "      <th>ActualLaneNum</th>\n",
       "      <th>LocationMile</th>\n",
       "      <th>DataCollectTime</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>VD-N5-N-0.178-M-LOOP</td>\n",
       "      <td>國道5號</td>\n",
       "      <td>南港系統交流道</td>\n",
       "      <td>石碇交流道</td>\n",
       "      <td>N</td>\n",
       "      <td>-99.0000</td>\n",
       "      <td>-99.0000</td>\n",
       "      <td>-99.0000</td>\n",
       "      <td>2</td>\n",
       "      <td>0.178</td>\n",
       "      <td>2023-01-01 00:04:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>VD-N5-N-0.706-M-LOOP</td>\n",
       "      <td>國道5號</td>\n",
       "      <td>南港系統交流道</td>\n",
       "      <td>石碇交流道</td>\n",
       "      <td>N</td>\n",
       "      <td>-99.0000</td>\n",
       "      <td>-99.0000</td>\n",
       "      <td>-99.0000</td>\n",
       "      <td>2</td>\n",
       "      <td>0.706</td>\n",
       "      <td>2023-01-01 00:04:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>VD-N5-N-1.068-M-LOOP</td>\n",
       "      <td>國道5號</td>\n",
       "      <td>南港系統交流道</td>\n",
       "      <td>石碇交流道</td>\n",
       "      <td>N</td>\n",
       "      <td>-99.0000</td>\n",
       "      <td>-99.0000</td>\n",
       "      <td>-99.0000</td>\n",
       "      <td>2</td>\n",
       "      <td>1.068</td>\n",
       "      <td>2023-01-01 00:04:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>VD-N5-N-2.068-M-PS-LOOP</td>\n",
       "      <td>國道5號</td>\n",
       "      <td>南港系統交流道</td>\n",
       "      <td>石碇交流道</td>\n",
       "      <td>N</td>\n",
       "      <td>94.6000</td>\n",
       "      <td>4.1000</td>\n",
       "      <td>5.3000</td>\n",
       "      <td>2</td>\n",
       "      <td>2.068</td>\n",
       "      <td>2023-01-01 00:04:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>VD-N5-N-3.198-M-LOOP</td>\n",
       "      <td>國道5號</td>\n",
       "      <td>南港系統交流道</td>\n",
       "      <td>石碇交流道</td>\n",
       "      <td>N</td>\n",
       "      <td>90.8000</td>\n",
       "      <td>4.8000</td>\n",
       "      <td>5.1000</td>\n",
       "      <td>2</td>\n",
       "      <td>3.198</td>\n",
       "      <td>2023-01-01 00:04:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9819329</th>\n",
       "      <td>VD-N5-S-41.298-M-LOOP</td>\n",
       "      <td>國道5號</td>\n",
       "      <td>宜蘭交流道</td>\n",
       "      <td>羅東交流道</td>\n",
       "      <td>S</td>\n",
       "      <td>92.5000</td>\n",
       "      <td>1.6667</td>\n",
       "      <td>2.6667</td>\n",
       "      <td>2</td>\n",
       "      <td>41.298</td>\n",
       "      <td>2023-12-31 23:57:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9819330</th>\n",
       "      <td>VD-N5-S-44.202-M-LOOP</td>\n",
       "      <td>國道5號</td>\n",
       "      <td>宜蘭交流道</td>\n",
       "      <td>羅東交流道</td>\n",
       "      <td>S</td>\n",
       "      <td>84.3333</td>\n",
       "      <td>2.3333</td>\n",
       "      <td>3.3333</td>\n",
       "      <td>2</td>\n",
       "      <td>44.202</td>\n",
       "      <td>2023-12-31 23:57:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9819331</th>\n",
       "      <td>VD-N5-S-46.566-M-LOOP</td>\n",
       "      <td>國道5號</td>\n",
       "      <td>宜蘭交流道</td>\n",
       "      <td>羅東交流道</td>\n",
       "      <td>S</td>\n",
       "      <td>65.8333</td>\n",
       "      <td>1.8333</td>\n",
       "      <td>1.5000</td>\n",
       "      <td>2</td>\n",
       "      <td>46.566</td>\n",
       "      <td>2023-12-31 23:57:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9819332</th>\n",
       "      <td>VD-N5-S-48.040-M-LOOP</td>\n",
       "      <td>國道5號</td>\n",
       "      <td>羅東交流道</td>\n",
       "      <td>蘇澳交流道</td>\n",
       "      <td>S</td>\n",
       "      <td>62.6667</td>\n",
       "      <td>0.6667</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>2</td>\n",
       "      <td>48.040</td>\n",
       "      <td>2023-12-31 23:57:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9819333</th>\n",
       "      <td>VD-N5-S-53.110-M-LOOP</td>\n",
       "      <td>國道5號</td>\n",
       "      <td>羅東交流道</td>\n",
       "      <td>蘇澳交流道</td>\n",
       "      <td>S</td>\n",
       "      <td>79.1667</td>\n",
       "      <td>1.5000</td>\n",
       "      <td>2.0000</td>\n",
       "      <td>2</td>\n",
       "      <td>53.110</td>\n",
       "      <td>2023-12-31 23:57:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9819334 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                            VDID RoadName    Start    End RoadDirection  \\\n",
       "0           VD-N5-N-0.178-M-LOOP     國道5號  南港系統交流道  石碇交流道             N   \n",
       "1           VD-N5-N-0.706-M-LOOP     國道5號  南港系統交流道  石碇交流道             N   \n",
       "2           VD-N5-N-1.068-M-LOOP     國道5號  南港系統交流道  石碇交流道             N   \n",
       "3        VD-N5-N-2.068-M-PS-LOOP     國道5號  南港系統交流道  石碇交流道             N   \n",
       "4           VD-N5-N-3.198-M-LOOP     國道5號  南港系統交流道  石碇交流道             N   \n",
       "...                          ...      ...      ...    ...           ...   \n",
       "9819329    VD-N5-S-41.298-M-LOOP     國道5號    宜蘭交流道  羅東交流道             S   \n",
       "9819330    VD-N5-S-44.202-M-LOOP     國道5號    宜蘭交流道  羅東交流道             S   \n",
       "9819331    VD-N5-S-46.566-M-LOOP     國道5號    宜蘭交流道  羅東交流道             S   \n",
       "9819332    VD-N5-S-48.040-M-LOOP     國道5號    羅東交流道  蘇澳交流道             S   \n",
       "9819333    VD-N5-S-53.110-M-LOOP     國道5號    羅東交流道  蘇澳交流道             S   \n",
       "\n",
       "           Speed  Occupancy   Volume  ActualLaneNum  LocationMile  \\\n",
       "0       -99.0000   -99.0000 -99.0000              2         0.178   \n",
       "1       -99.0000   -99.0000 -99.0000              2         0.706   \n",
       "2       -99.0000   -99.0000 -99.0000              2         1.068   \n",
       "3        94.6000     4.1000   5.3000              2         2.068   \n",
       "4        90.8000     4.8000   5.1000              2         3.198   \n",
       "...          ...        ...      ...            ...           ...   \n",
       "9819329  92.5000     1.6667   2.6667              2        41.298   \n",
       "9819330  84.3333     2.3333   3.3333              2        44.202   \n",
       "9819331  65.8333     1.8333   1.5000              2        46.566   \n",
       "9819332  62.6667     0.6667   1.0000              2        48.040   \n",
       "9819333  79.1667     1.5000   2.0000              2        53.110   \n",
       "\n",
       "            DataCollectTime  \n",
       "0       2023-01-01 00:04:00  \n",
       "1       2023-01-01 00:04:00  \n",
       "2       2023-01-01 00:04:00  \n",
       "3       2023-01-01 00:04:00  \n",
       "4       2023-01-01 00:04:00  \n",
       "...                     ...  \n",
       "9819329 2023-12-31 23:57:00  \n",
       "9819330 2023-12-31 23:57:00  \n",
       "9819331 2023-12-31 23:57:00  \n",
       "9819332 2023-12-31 23:57:00  \n",
       "9819333 2023-12-31 23:57:00  \n",
       "\n",
       "[9819334 rows x 11 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = []\n",
    "for filename in os.listdir('./nfb2023'):\n",
    "    monthlyDf = feather.read_dataframe(f\"./nfb2023/{filename}\")\n",
    "    if (len(df) == 0):\n",
    "        df.append(monthlyDf)\n",
    "    else:\n",
    "        currDf = pd.concat(df).reset_index(drop=True)\n",
    "        monthlyDf = monthlyDf.loc[monthlyDf['VDID'].isin(set(currDf['VDID']))]\n",
    "        df.append(monthlyDf)\n",
    "df = pd.concat(df).reset_index(drop=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('./df.csv', index=False, encoding='utf_8_sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # TODO: main\n",
    "# if __name__ == '__main__':\n",
    "#     # # read feather files to get dataframes\n",
    "#     # startDate = '2023-01-01'\n",
    "#     # endDate = getEndDate(startDate, days=10)\n",
    "#     # # df = getRollingMean(startDate, endDate)\n",
    "#     # df = feather.read_dataframe('./20230101-20230110.feather').sort_values(by=['RoadDirection','DataCollectTime','LocationMile']).reset_index(drop=True)\n",
    "    \n",
    "#     # Northbound data\n",
    "#     northDf = df.loc[df['RoadDirection']=='N'].reset_index(drop=True)\n",
    "#     each = 3\n",
    "#     vdGroups = groupVDs(northDf, each)    \n",
    "#     speedDataset, volDataset, occDataset = [], [], []\n",
    "#     for groupKey in vdGroups.keys():\n",
    "#         speeds, vols, occs = genSamples(northDf, vdGroups, groupKey, each, timeWindow=30)\n",
    "#         speedDataset.append(speeds)\n",
    "#         volDataset.append(vols)\n",
    "#         occDataset.append(occs)\n",
    "\n",
    "#     # Southbound data\n",
    "#     southDf = df.loc[df['RoadDirection']=='S'].reset_index(drop=True)\n",
    "#     each = 3\n",
    "#     vdGroups = groupVDs(southDf, each)    \n",
    "#     speedDataset, volDataset, occDataset = [], [], []\n",
    "#     for groupKey in vdGroups.keys():\n",
    "#         speeds, vols, occs = genSamples(southDf, vdGroups, groupKey, each, timeWindow=30)\n",
    "#         speedDataset.append(speeds)\n",
    "#         volDataset.append(vols)\n",
    "#         occDataset.append(occs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## test cell for missing data\n",
    "# This part is genSamples()\n",
    "each = 3\n",
    "timeWindow = 30\n",
    "\n",
    "# df = feather.read_dataframe(\"./nfb2023/202305.feather\")\n",
    "northDf = df.loc[df['RoadDirection']=='N'].reset_index(drop=True)\n",
    "vdGroups = groupVDs(northDf, each)\n",
    "groupKey = 'VD-N5-N-1.068-M-LOOP'\n",
    "\n",
    "\n",
    "\n",
    "speeds, vols, occs = [], [], []\n",
    "tmpDf = df.loc[(df['VDID'].isin(vdGroups[f\"{groupKey}\"]))].sort_values(by=['LocationMile', 'DataCollectTime'])\n",
    "\n",
    "indices = [x for x in range(0, tmpDf.shape[0]+1, tmpDf.shape[0]//each)]\n",
    "mileMatx = np.zeros((each, tmpDf.shape[0]//each))\n",
    "speedMatx = np.zeros((each, tmpDf.shape[0]//each))\n",
    "volMatx = np.zeros((each, tmpDf.shape[0]//each))\n",
    "occMatx = np.zeros((each, tmpDf.shape[0]//each))\n",
    "for i, j, k in zip(range(each), indices[:-1], indices[1:]):\n",
    "    mileMatx[i] += tmpDf.iloc[j:k,:]['LocationMile'].to_numpy()\n",
    "    speedMatx[i] += tmpDf.iloc[j:k,:]['Speed'].to_numpy()\n",
    "    volMatx[i] += tmpDf.iloc[j:k,:]['Volume'].to_numpy()\n",
    "    occMatx[i] += tmpDf.iloc[j:k,:]['Occupancy'].to_numpy()\n",
    "\n",
    "# sliceLen = int((timeWindow / 5) + 1)\n",
    "# for x in range(speedMatx.shape[1]//sliceLen*sliceLen-(sliceLen-1)):\n",
    "#     speeds.append((speedMatx[:,x:x+sliceLen][:,:-1], speedMatx[:,x:x+sliceLen][:,[-1]]))\n",
    "#     vols.append((volMatx[:,x:x+sliceLen][:,:-1], volMatx[:,x:x+sliceLen][:,[-1]]))\n",
    "#     occs.append((occMatx[:,x:x+sliceLen][:,:-1], occMatx[:,x:x+sliceLen][:,[-1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mileMatx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testDf['LocationMile'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testDf = northDf.sort_values(by=['LocationMile', 'DataCollectTime'])\n",
    "n = testDf['LocationMile'].nunique()\n",
    "\n",
    "indices__ = [x for x in range(0, testDf.shape[0]+1, testDf.shape[0]//n)]\n",
    "mileMatx__ = np.zeros((n, testDf.shape[0]//n))\n",
    "speedMatx__ = np.zeros((n, testDf.shape[0]//n))\n",
    "volMatx__ = np.zeros((n, testDf.shape[0]//n))\n",
    "occMatx__ = np.zeros((n, testDf.shape[0]//n))\n",
    "for i, j, k in zip(range(n), indices__[:-1], indices__[1:]):\n",
    "    mileMatx__[i] += testDf.iloc[j:k,:]['LocationMile'].to_numpy()\n",
    "    speedMatx__[i] += testDf.iloc[j:k,:]['Speed'].to_numpy()\n",
    "    volMatx__[i] += testDf.iloc[j:k,:]['Volume'].to_numpy()\n",
    "    occMatx__[i] += testDf.iloc[j:k,:]['Occupancy'].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = mileMatx__[:5,:5].copy()\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m - np.ones(m.shape) * m[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speedMatx__[:5,:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "volMatx__[:5,:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speedMatx[:,:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "volMatx[:,:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "occMatx[:,:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Dataset for CNN (Short-term Prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "northDf start grouping: 2024-03-27 23:09:53\n",
      "northDf end grouping: 2024-03-27 23:11:12\n",
      "VD-N5-N-0.706-M-LOOP\n",
      "VD-N5-N-1.068-M-LOOP\n",
      "VD-N5-N-2.068-M-PS-LOOP\n",
      "VD-N5-N-3.198-M-LOOP\n",
      "VD-N5-N-3.943-M-LOOP\n",
      "VD-N5-N-5.883-M-LOOP\n",
      "VD-N5-N-7.107-M-LOOP\n",
      "VD-N5-N-8.011-M-LOOP\n",
      "VD-N5-N-9.840-M-LOOP\n",
      "VD-N5-N-10.866-M-PS-LOOP\n",
      "VD-N5-N-11.903-M-PS-LOOP\n",
      "VD-N5-N-12.922-M-LOOP\n",
      "VD-N5-N-13.707-M-LOOP\n",
      "VD-N5-N-14.550-M-LOOP\n",
      "VD-N5-N-15.488-M-LOOP\n",
      "VD-N5-N-16.196-M-LOOP\n",
      "VD-N5-N-16.900-M-PS-LOOP\n",
      "VD-N5-N-17.608-M-LOOP\n",
      "VD-N5-N-18.313-M-PS-LOOP\n",
      "VD-N5-N-19.012-M-LOOP\n",
      "VD-N5-N-19.689-M-PS-LOOP\n",
      "VD-N5-N-20.412-M-LOOP\n",
      "VD-N5-N-21.055-M-PS-LOOP\n",
      "VD-N5-N-21.808-M-LOOP\n",
      "VD-N5-N-22.510-M-PS-LOOP\n",
      "VD-N5-N-23.209-M-LOOP\n",
      "VD-N5-N-23.911-M-PS-LOOP\n",
      "VD-N5-N-24.677-M-LOOP\n",
      "VD-N5-N-25.310-M-PS-LOOP\n",
      "VD-N5-N-26.007-M-LOOP\n",
      "VD-N5-N-26.705-M-PS-LOOP\n",
      "VD-N5-N-27.468-M-LOOP\n",
      "VD-N5-N-27.779-M-LOOP\n",
      "VD-N5-N-28.420-M-LOOP\n",
      "VD-N5-N-29.000-M-LOOP\n",
      "VD-N5-N-30.100-M-LOOP\n",
      "VD-N5-N-30.551-M-LOOP\n",
      "VD-N5-N-31.540-M-LOOP\n",
      "VD-N5-N-32.120-M-LOOP\n",
      "VD-N5-N-32.743-M-LOOP\n",
      "VD-N5-N-33.650-M-LOOP\n",
      "VD-N5-N-34.898-M-LOOP\n",
      "VD-N5-N-36.073-M-LOOP\n",
      "VD-N5-N-37.225-M-LOOP\n",
      "VD-N5-N-42.359-M-LOOP\n",
      "VD-N5-N-45.230-M-LOOP\n",
      "southDf start grouping: 2024-03-27 23:13:23\n",
      "southDf end grouping: 2024-03-27 23:14:33\n",
      "VD-N5-S-1.072-M-LOOP\n",
      "VD-N5-S-2.050-M-PS-LOOP\n",
      "VD-N5-S-3.178-M-LOOP\n",
      "VD-N5-S-3.506-M-LOOP\n",
      "VD-N5-S-4.278-M-LOOP\n",
      "VD-N5-S-4.633-M-LOOP\n",
      "VD-N5-S-7.577-M-LOOP\n",
      "VD-N5-S-8.686-M-LOOP\n",
      "VD-N5-S-9.334-M-LOOP\n",
      "VD-N5-S-9.840-M-LOOP\n",
      "VD-N5-S-10.845-M-PS-LOOP\n",
      "VD-N5-S-11.903-M-PS-LOOP\n",
      "VD-N5-S-12.945-M-LOOP\n",
      "VD-N5-S-13.343-M-LOOP\n",
      "VD-N5-S-14.540-M-LOOP\n",
      "VD-N5-S-15.139-M-LOOP\n",
      "VD-N5-S-15.478-M-LOOP\n",
      "VD-N5-S-16.187-M-LOOP\n",
      "VD-N5-S-16.902-M-PS-LOOP\n",
      "VD-N5-S-17.608-M-LOOP\n",
      "VD-N5-S-18.312-M-PS-LOOP\n",
      "VD-N5-S-19.013-M-LOOP\n",
      "VD-N5-S-19.677-M-PS-LOOP\n",
      "VD-N5-S-20.413-M-LOOP\n",
      "VD-N5-S-21.063-M-PS-LOOP\n",
      "VD-N5-S-21.807-M-LOOP\n",
      "VD-N5-S-22.506-M-PS-LOOP\n",
      "VD-N5-S-23.207-M-LOOP\n",
      "VD-N5-S-23.910-M-PS-LOOP\n",
      "VD-N5-S-24.678-M-LOOP\n",
      "VD-N5-S-25.312-M-PS-LOOP\n",
      "VD-N5-S-26.013-M-LOOP\n",
      "VD-N5-S-26.706-M-PS-LOOP\n",
      "VD-N5-S-27.442-M-LOOP\n",
      "VD-N5-S-27.748-M-LOOP\n",
      "VD-N5-S-28.236-M-LOOP\n",
      "VD-N5-S-30.266-M-LOOP\n",
      "VD-N5-S-34.853-M-LOOP\n",
      "VD-N5-S-39.776-M-LOOP\n",
      "VD-N5-S-41.298-M-LOOP\n",
      "VD-N5-S-44.202-M-LOOP\n",
      "VD-N5-S-46.566-M-LOOP\n"
     ]
    }
   ],
   "source": [
    "EACH = 3\n",
    "speedCollection, volCollection, occCollection = [], [], []\n",
    "\n",
    "# Northbound data\n",
    "northDf = df.loc[df['RoadDirection']=='N'].reset_index(drop=True)\n",
    "print(f\"northDf start grouping: {datetime.strftime(datetime.now(), '%Y-%m-%d %H:%M:%S')}\")\n",
    "northVDGrps = groupVDs(northDf, each=EACH)\n",
    "print(f\"northDf end grouping: {datetime.strftime(datetime.now(), '%Y-%m-%d %H:%M:%S')}\")\n",
    "for groupKey in northVDGrps.keys():\n",
    "    print(groupKey)\n",
    "    speeds, vols, occs = genSamples(northDf, northVDGrps, groupKey, each=EACH, timeWindow=30)\n",
    "    speedCollection += speeds\n",
    "    volCollection += vols\n",
    "    occCollection += occs\n",
    "\n",
    "# Southbound data\n",
    "southDf = df.loc[df['RoadDirection']=='S'].reset_index(drop=True)\n",
    "print(f\"southDf start grouping: {datetime.strftime(datetime.now(), '%Y-%m-%d %H:%M:%S')}\")\n",
    "southVDGrps = groupVDs(southDf, each=EACH)\n",
    "print(f\"southDf end grouping: {datetime.strftime(datetime.now(), '%Y-%m-%d %H:%M:%S')}\")\n",
    "# speedDataset, volDataset, occDataset = [], [], []\n",
    "for groupKey in southVDGrps.keys():\n",
    "    print(groupKey)\n",
    "    speeds, vols, occs = genSamples(southDf, southVDGrps, groupKey, each=EACH, timeWindow=30)\n",
    "    speedCollection += speeds\n",
    "    volCollection += vols\n",
    "    occCollection += occs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNDataset(Dataset):\n",
    "    def __init__(\n",
    "            self,\n",
    "            speed_data: list = None,\n",
    "            volume_data: list = None,\n",
    "            load_ckpt: bool = None,\n",
    "            mode: str = None,\n",
    "            ckpt_dir: str = './datasets/cnndataset'\n",
    "    ) -> None:\n",
    "        if (speed_data):\n",
    "            self.speedFeature = [speed_data[x][0] for x in range(len(speed_data))]\n",
    "            self.volFeature = [volume_data[x][0] for x in range(len(volume_data))]\n",
    "            self.speedLabels = [speed_data[x][1][[1],:] for x in range(len(speed_data))]\n",
    "            self.volLabels = [volume_data[x][1][[1],:] for x in range(len(volume_data))]\n",
    "        \n",
    "        else:\n",
    "            if (load_ckpt) and (mode == 'train'):\n",
    "                with h5py.File(f\"{ckpt_dir}/{mode}/{mode}_speed_feature.h5\", 'r') as file:\n",
    "                    self.speedFeature = file[f\"{mode}_speed_feature\"][:]\n",
    "                with h5py.File(f\"{ckpt_dir}/{mode}/{mode}_volume_feature.h5\", 'r') as file:\n",
    "                    self.volFeature = file[f\"{mode}_volume_feature\"][:]\n",
    "                with h5py.File(f\"{ckpt_dir}/{mode}/{mode}_speed_label.h5\", 'r') as file:\n",
    "                    self.speedLabels = file[f\"{mode}_speed_label\"][:]\n",
    "                with h5py.File(f\"{ckpt_dir}/{mode}/{mode}_volume_label.h5\", 'r') as file:\n",
    "                    self.volLabels = file[f\"{mode}_volume_label\"][:]\n",
    "            \n",
    "            elif (load_ckpt) and (mode == 'test'):\n",
    "                with h5py.File(f\"{ckpt_dir}/{mode}/{mode}_speed_feature.h5\", 'r') as file:\n",
    "                    self.speedFeature = file[f\"{mode}_speed_feature\"][:]\n",
    "                with h5py.File(f\"{ckpt_dir}/{mode}/{mode}_volume_feature.h5\", 'r') as file:\n",
    "                    self.volFeature = file[f\"{mode}_volume_feature\"][:]\n",
    "                with h5py.File(f\"{ckpt_dir}/{mode}/{mode}_speed_label.h5\", 'r') as file:\n",
    "                    self.speedLabels = file[f\"{mode}_speed_label\"][:]\n",
    "                with h5py.File(f\"{ckpt_dir}/{mode}/{mode}_volume_label.h5\", 'r') as file:\n",
    "                    self.volLabels = file[f\"{mode}_volume_label\"][:]\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.speedFeature)\n",
    "    \n",
    "    def __getitem__(self, idx: int) -> torch.Tensor:\n",
    "        f1 = torch.tensor(self.speedFeature[idx], dtype=torch.float).unsqueeze(0).unsqueeze(0)\n",
    "        f2 = torch.tensor(self.volFeature[idx], dtype=torch.float).unsqueeze(0).unsqueeze(0)\n",
    "        l1 = torch.tensor(self.speedLabels[idx], dtype=torch.float).unsqueeze(0)\n",
    "        l2 = torch.tensor(self.volLabels[idx], dtype=torch.float).unsqueeze(0)\n",
    "        feature = torch.concat([f1, f2], dim=1)\n",
    "        label = torch.concat([l1, l2], dim=1)\n",
    "        return feature, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainSpeed, trainVol, testSpeed, testVol =\\\n",
    "    train_test_split(speedCollection, volCollection, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDataset = CNNDataset(speed_data=trainSpeed, volume_data=trainVol)\n",
    "testDataset = CNNDataset(speed_data=testSpeed, volume_data=testVol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "tedo = CNNDataset(speed_data=testSpeed, volume_data=testVol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1838408"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tedo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save dataset as `.h5`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File('./datasets/cnndataset/train_speed_feature.h5', 'w') as f:\n",
    "    f.create_dataset('train_speed_feature', data=trainDataset.speedFeature)\n",
    "\n",
    "with h5py.File('./datasets/cnndataset/train_volume_feature.h5', 'w') as f:\n",
    "    f.create_dataset('train_volume_feature', data=trainDataset.volFeature)\n",
    "\n",
    "with h5py.File('./datasets/cnndataset/train_speed_label.h5', 'w') as f:\n",
    "    f.create_dataset('train_speed_label', data=trainDataset.speedLabels)\n",
    "\n",
    "with h5py.File('./datasets/cnndataset/train_volume_label.h5', 'w') as f:\n",
    "    f.create_dataset('train_volume_label', data=trainDataset.volLabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File('./datasets/cnndataset/test_speed_feature.h5', 'w') as f:\n",
    "    f.create_dataset('test_speed_feature', data=testDataset.speedFeature)\n",
    "\n",
    "with h5py.File('./datasets/cnndataset/test_volume_feature.h5', 'w') as f:\n",
    "    f.create_dataset('test_volume_feature', data=testDataset.volFeature)\n",
    "\n",
    "with h5py.File('./datasets/cnndataset/test_speed_label.h5', 'w') as f:\n",
    "    f.create_dataset('test_speed_label', data=testDataset.speedLabels)\n",
    "\n",
    "with h5py.File('./datasets/cnndataset/test_volume_label.h5', 'w') as f:\n",
    "    f.create_dataset('test_volume_label', data=testDataset.volLabels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Directly load dataset\n",
    "\n",
    "You can also load datasets from `.h5` file if you have saved them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDataset = CNNDataset(load_ckpt=True, mode='train', ckpt_dir='./datasets/cnndataset')\n",
    "testDataset = CNNDataset(load_ckpt=True, mode='test', ckpt_dir='./datasets/cnndataset')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "route-plan",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
