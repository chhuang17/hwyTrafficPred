{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Home\\anaconda3\\envs\\route-plan\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from sqlalchemy import create_engine\n",
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from dotenv import load_dotenv\n",
    "from toolkits.datapreparing import download_monthly_tables, collect_data\n",
    "from toolkits.datasets import CNNDataset, train_test_split, load_next_5min\n",
    "from tqdm.auto import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import feather\n",
    "import os\n",
    "import h5py\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "load_dotenv('./.env')\n",
    "engine = create_engine(os.getenv('DB_ENGINE'))\n",
    "\n",
    "\n",
    "# def getEndDate(startDate: str, days: int) -> str:\n",
    "#     startDate += ' 00:00:00'\n",
    "#     end = str((datetime.strptime(startDate, '%Y-%m-%d %H:%M:%S') + timedelta(days=days)).replace(microsecond=0))\n",
    "#     return end\n",
    "\n",
    "# def getRollingMeanDaily(selectDate: str) -> pd.DataFrame:\n",
    "#     sql  = \" SELECT \"\n",
    "#     sql += \" \tSTAC.VDID, STAC.RoadName, STAC.`Start`, STAC.`End`, STAC.RoadDirection, \"\n",
    "#     sql += \"    CASE \"\n",
    "#     sql += \"        WHEN DYMC.Occupancy = 0 AND DYMC.Volume = 0 THEN 100 \"\n",
    "#     sql += \"        ELSE DYMC.Speed \"\n",
    "#     sql += \" \tEND AS Speed, \"\n",
    "#     sql += \"    DYMC.Occupancy, DYMC.Volume, \"\n",
    "#     sql += \" \tSTAC.ActualLaneNum, STAC.LocationMile, STAC.isTunnel, DYMC.DataCollectTime \"\n",
    "#     sql += \" FROM ( \"\n",
    "#     sql += \" \tSELECT \"\n",
    "#     sql += \" \t\tVDSTC.id, VDSTC.VDID, ROAD.RoadName, SEC.`Start`, SEC.`End`, \"\n",
    "#     sql += \" \t\tVDSTC.ActualLaneNum, VDSTC.RoadDirection, VDSTC.LocationMile, \"\n",
    "#     sql += \"        CASE \"\n",
    "#     sql += \" \t        WHEN VDSTC.RoadDirection = 'S' AND VDSTC.LocationMile BETWEEN 0.238 AND 0.694 THEN 1 \"\n",
    "#     sql += \" \t        WHEN VDSTC.RoadDirection = 'N' AND VDSTC.LocationMile BETWEEN 0.235 AND 0.690 THEN 1 \"\n",
    "#     sql += \" \t        WHEN VDSTC.RoadDirection = 'S' AND VDSTC.LocationMile BETWEEN 0.694 AND 3.481 THEN 1 \"\n",
    "#     sql += \" \t        WHEN VDSTC.RoadDirection = 'N' AND VDSTC.LocationMile BETWEEN 0.795 AND 3.515 THEN 1 \"\n",
    "#     sql += \" \t        WHEN VDSTC.RoadDirection = 'S' AND VDSTC.LocationMile BETWEEN 7.677 AND 7.893 THEN 1 \"\n",
    "#     sql += \" \t        WHEN VDSTC.RoadDirection = 'N' AND VDSTC.LocationMile BETWEEN 7.646 AND 7.894 THEN 1 \"\n",
    "#     sql += \" \t        WHEN VDSTC.RoadDirection = 'S' AND VDSTC.LocationMile BETWEEN 9.442 AND 13.303 THEN 1 \"\n",
    "#     sql += \" \t        WHEN VDSTC.RoadDirection = 'N' AND VDSTC.LocationMile BETWEEN 9.457 AND 13.263 THEN 1 \"\n",
    "#     sql += \" \t        WHEN VDSTC.RoadDirection = 'S' AND VDSTC.LocationMile BETWEEN 15.203 AND 28.128 THEN 1 \"\n",
    "#     sql += \" \t        WHEN VDSTC.RoadDirection = 'N' AND VDSTC.LocationMile BETWEEN 15.179 AND 28.134 THEN 1 \"\n",
    "#     sql += \" \t        ELSE 0 \"\n",
    "#     sql += \"        END AS isTunnel \"\n",
    "#     sql += \" \tFROM fwy_n5.vd_static_2023 VDSTC \"\n",
    "#     sql += \" \tJOIN transport.road_info ROAD ON VDSTC.RoadInfoID = ROAD.id \"\n",
    "#     sql += \" \tJOIN transport.section_info SEC ON ROAD.id = SEC.RoadInfoID \"\n",
    "#     sql += \" \tAND VDSTC.LocationMile >= SEC.StartKM \"\n",
    "#     sql += \" \tAND VDSTC.LocationMile <= SEC.EndKM \"\n",
    "#     sql += \" \tWHERE VDSTC.Mainlane = 1 \"\n",
    "#     sql += \" ) STAC JOIN ( \"\n",
    "#     sql += \" \tSELECT \"\n",
    "#     sql += \" \t\tVdStaticID, \"\n",
    "#     sql += \" \t\tCASE \"\n",
    "#     sql += \" \t\t\tWHEN MIN(Speed) = -99 THEN -99 \"\n",
    "#     sql += \" \t\t\tELSE AVG(Speed) \"\n",
    "#     sql += \" \t\tEND AS Speed,  \"\n",
    "#     sql += \" \t\tCASE \"\n",
    "#     sql += \" \t\t\tWHEN MIN(Occupancy) = -99 THEN -99 \"\n",
    "#     sql += \" \t\t\tELSE AVG(Occupancy) \"\n",
    "#     sql += \" \t\tEND AS Occupancy,  \"\n",
    "#     sql += \" \t\tCASE \"\n",
    "#     sql += \" \t\t\tWHEN MIN(Volume) = -99 THEN -99 \"\n",
    "#     sql += \" \t\t\tELSE AVG(Volume) \"\n",
    "#     sql += \" \t\tEND AS Volume, \"\n",
    "#     sql += \" \t\tMAX(DataCollectTime) AS DataCollectTime, \"\n",
    "#     sql += \" \t\t(UNIX_TIMESTAMP(DataCollectTime)-UNIX_TIMESTAMP(%(selectDate)s)) DIV 300 \"\n",
    "#     sql += \" \tFROM fwy_n5.vd_dynamic_detail_{} \".format(selectDate.replace('-',''))\n",
    "#     sql += \" \tGROUP BY VdStaticID, (UNIX_TIMESTAMP(DataCollectTime)-UNIX_TIMESTAMP(%(selectDate)s)) DIV 300 \"\n",
    "#     sql += \" ) DYMC ON STAC.id = DYMC.VdStaticID \"\n",
    "#     sql += \" ORDER BY STAC.RoadDirection, STAC.LocationMile, DYMC.DataCollectTime; \"\n",
    "\n",
    "#     df = pd.read_sql(sql, con=engine, params={'selectDate': selectDate})\n",
    "#     engine.dispose()\n",
    "#     return df.sort_values(by=['RoadDirection','DataCollectTime','LocationMile']).reset_index(drop=True)\n",
    "\n",
    "# def groupVDs(df: pd.DataFrame, each: int) -> dict:\n",
    "#     \"\"\" Get the dict of VD groups\n",
    "#         ```text\n",
    "#         ---\n",
    "#         @Params\n",
    "#         df: DataFrame which is referenced by.\n",
    "#         each: The quantity of VDs would be considered as a group.\n",
    "\n",
    "#         ---\n",
    "#         @Returns\n",
    "#         vdGroups: The keys are the VDs we focus on, and the values are the collections of VDs which are correlated corresponding to the keys.\n",
    "#         ```\n",
    "#     \"\"\"\n",
    "#     vdGroups = {}\n",
    "#     lb = each // 2\n",
    "#     ub = each - (each // 2)\n",
    "#     for vdid in df['VDID'].unique():\n",
    "#         vdGroups.setdefault(f\"{vdid}\", [])\n",
    "#     for no, vdid in enumerate(df['VDID'].unique()):\n",
    "#         startIdx = max(no-lb, 0)\n",
    "#         endIdx = min(no+ub, len(df['VDID'].unique())-1)\n",
    "#         vdGroups[f\"{vdid}\"] += list(df['VDID'].unique()[startIdx:no]) + list(df['VDID'].unique()[no:endIdx])\n",
    "\n",
    "#     delList = []\n",
    "#     for k in vdGroups.keys():\n",
    "#         if (len(vdGroups[k]) != each):\n",
    "#             delList.append(k)\n",
    "#     for k in delList:\n",
    "#         del vdGroups[k]\n",
    "    \n",
    "#     return vdGroups\n",
    "\n",
    "# def genSamples(df: pd.DataFrame, vdGroups: dict, groupKey: str, each: int, timeWindow: int = 30) -> tuple:\n",
    "#     \"\"\" Generate samples for each traffic data (speed, volume, and occupancy)\n",
    "#         ```text\n",
    "#         ---\n",
    "#         @Params\n",
    "#         df: \n",
    "#         vdGroups: The outpur of groupVDs(),\n",
    "#         groupKey: The key of vdGroups,\n",
    "#         each: The quantity of VDs would be considered as a group,\n",
    "#         timeWindow: The length of period we consider, and the default value is 30 (minutes).\n",
    "\n",
    "#         ---\n",
    "#         @Returns\n",
    "#         speeds: list with each item as a tuple, all of them are represented (X,y).\n",
    "#         vols: list with each item as a tuple, all of them are represented (X,y).\n",
    "#         occs: list with each item as a tuple, all of them are represented (X,y).\n",
    "#         ```\n",
    "#     \"\"\"\n",
    "#     speeds, vols, occs, lanes, tunnels = [], [], [], [], []\n",
    "#     tmpDf = df.loc[(df['VDID'].isin(vdGroups[f\"{groupKey}\"]))].sort_values(by=['LocationMile', 'DataCollectTime'])\n",
    "\n",
    "#     indices = [x for x in range(0, tmpDf.shape[0]+1, tmpDf.shape[0]//each)]\n",
    "#     speedMatx = np.zeros((each, tmpDf.shape[0]//each))\n",
    "#     volMatx = np.zeros((each, tmpDf.shape[0]//each))\n",
    "#     occMatx = np.zeros((each, tmpDf.shape[0]//each))\n",
    "#     laneMatx = np.zeros((each, tmpDf.shape[0]//each))\n",
    "#     tunnelMatx = np.zeros((each, tmpDf.shape[0]//each))\n",
    "#     for i, j, k in zip(range(each), indices[:-1], indices[1:]):\n",
    "#         speedMatx[i] += tmpDf.iloc[j:k,:]['Speed'].to_numpy()\n",
    "#         volMatx[i] += tmpDf.iloc[j:k,:]['Volume'].to_numpy()\n",
    "#         occMatx[i] += tmpDf.iloc[j:k,:]['Occupancy'].to_numpy()\n",
    "#         laneMatx[i] += tmpDf.iloc[j:k,:]['ActualLaneNum'].to_numpy()\n",
    "#         tunnelMatx[i] += tmpDf.iloc[j:k,:]['isTunnel'].to_numpy()\n",
    "\n",
    "#     sliceLen = int((timeWindow / 5) + 1)\n",
    "#     for x in range(speedMatx.shape[1]//sliceLen*sliceLen-(sliceLen-1)):\n",
    "#         speeds.append((speedMatx[:,x:x+sliceLen][:,:-1], speedMatx[:,x:x+sliceLen][:,[-1]]))\n",
    "#         vols.append((volMatx[:,x:x+sliceLen][:,:-1], volMatx[:,x:x+sliceLen][:,[-1]]))\n",
    "#         occs.append((occMatx[:,x:x+sliceLen][:,:-1], occMatx[:,x:x+sliceLen][:,[-1]]))\n",
    "#         lanes.append((laneMatx[:,x:x+sliceLen][:,:-1], laneMatx[:,x:x+sliceLen][:,[-1]]))\n",
    "#         tunnels.append((occMatx[:,x:x+sliceLen][:,:-1], tunnelMatx[:,x:x+sliceLen][:,[-1]]))\n",
    "    \n",
    "#     return speeds, vols, occs, lanes, tunnels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## main\n",
    "\n",
    "# download_monthly_tables(start='2023-01-01', end='2023-12-31', dest_dir='./nfb2023', file_format='feather')\n",
    "speedCollection, volCollection, occCollection, laneCollection, tunnelCollection = collect_data()\n",
    "\n",
    "trainSpeed, trainVol, trainOcc, trainNumLane, trainTunnel,\\\n",
    "testSpeed, testVol, testOcc, testNumLane, testTunnel =\\\n",
    "    train_test_split(speedCollection, volCollection, occCollection, laneCollection, tunnelCollection, test_size=0.2)\n",
    "\n",
    "trainDataset = CNNDataset(speed_data=trainSpeed, volume_data=trainVol, occupy_data=trainOcc,\n",
    "                          lane_data=trainNumLane, tunnel_data=trainTunnel, load_ckpt=False, mode='train')\n",
    "testDataset = CNNDataset(speed_data=testSpeed, volume_data=testVol, occupy_data=testOcc,\n",
    "                         lane_data=testNumLane, tunnel_data=testTunnel, load_ckpt=False, mode='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_feather('./nfb2023/202301.feather')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 3, 6])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainDataset[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 取得一年份資料\n",
    "firstDate = list(map(lambda x: datetime.strftime(x, '%Y-%m-%d'), list(pd.date_range('2023-01-01', '2023-12-31', freq='MS'))))\n",
    "lastDate = list(map(lambda x: datetime.strftime(x, '%Y-%m-%d'), list(pd.date_range('2023-01-01', '2023-12-31', freq='ME'))))\n",
    "for first, last in zip(firstDate, lastDate):\n",
    "    dataframes = []\n",
    "    dateList = list(map(lambda x: datetime.strftime(x, '%Y-%m-%d'), list(pd.date_range(first, last))))\n",
    "    for date in dateList:\n",
    "        print(date)\n",
    "        dataframes.append(getRollingMeanDaily(date))\n",
    "    dataframes = pd.concat(dataframes).reset_index(drop=True)\n",
    "    # display(dataframes)\n",
    "    feather.write_dataframe(dataframes, dest=f\"./nfb2023/{date[:7].replace('-','')}.feather\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "monthlyStarts = list(map(lambda x: datetime.strftime(x, '%Y-%m-%d'), list(pd.date_range('2023-02-01', '2023-02-28', freq='MS'))))\n",
    "monthlyEnds = list(map(lambda x: datetime.strftime(x, '%Y-%m-%d'), list(pd.date_range('2023-02-01', '2023-02-28', freq='ME'))))\n",
    "\n",
    "for start, end in zip(monthlyStarts, monthlyEnds):\n",
    "    dataframes = []\n",
    "    dateList = list(map(lambda x: datetime.strftime(x, '%Y-%m-%d'), list(pd.date_range(start, end))))\n",
    "    print(start[:7].replace('-',''))\n",
    "    for date in dateList:\n",
    "        print(date)\n",
    "        dataframes.append(getRollingMeanDaily(date))\n",
    "    dataframes = pd.concat(dataframes).reset_index(drop=True)\n",
    "    # display(dataframes)\n",
    "    feather.write_dataframe(dataframes, dest=f\"./nfb2023/{start[:7].replace('-','')}.feather\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.listdir('./nfb2023')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = []\n",
    "for filename in os.listdir('./nfb2023'):\n",
    "    monthlyDf = feather.read_dataframe(f\"./nfb2023/{filename}\")\n",
    "    if (len(df) == 0):\n",
    "        df.append(monthlyDf)\n",
    "    else:\n",
    "        currDf = pd.concat(df).reset_index(drop=True)\n",
    "        monthlyDf = monthlyDf.loc[monthlyDf['VDID'].isin(set(currDf['VDID']))]\n",
    "        df.append(monthlyDf)\n",
    "df = pd.concat(df).reset_index(drop=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('./df.csv', index=False, encoding='utf_8_sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # TODO: main\n",
    "# if __name__ == '__main__':\n",
    "#     # # read feather files to get dataframes\n",
    "#     # startDate = '2023-01-01'\n",
    "#     # endDate = getEndDate(startDate, days=10)\n",
    "#     # # df = getRollingMean(startDate, endDate)\n",
    "#     # df = feather.read_dataframe('./20230101-20230110.feather').sort_values(by=['RoadDirection','DataCollectTime','LocationMile']).reset_index(drop=True)\n",
    "    \n",
    "#     # Northbound data\n",
    "#     northDf = df.loc[df['RoadDirection']=='N'].reset_index(drop=True)\n",
    "#     each = 3\n",
    "#     vdGroups = groupVDs(northDf, each)    \n",
    "#     speedDataset, volDataset, occDataset = [], [], []\n",
    "#     for groupKey in vdGroups.keys():\n",
    "#         speeds, vols, occs = genSamples(northDf, vdGroups, groupKey, each, timeWindow=30)\n",
    "#         speedDataset.append(speeds)\n",
    "#         volDataset.append(vols)\n",
    "#         occDataset.append(occs)\n",
    "\n",
    "#     # Southbound data\n",
    "#     southDf = df.loc[df['RoadDirection']=='S'].reset_index(drop=True)\n",
    "#     each = 3\n",
    "#     vdGroups = groupVDs(southDf, each)    \n",
    "#     speedDataset, volDataset, occDataset = [], [], []\n",
    "#     for groupKey in vdGroups.keys():\n",
    "#         speeds, vols, occs = genSamples(southDf, vdGroups, groupKey, each, timeWindow=30)\n",
    "#         speedDataset.append(speeds)\n",
    "#         volDataset.append(vols)\n",
    "#         occDataset.append(occs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## test cell for missing data\n",
    "# This part is genSamples()\n",
    "each = 3\n",
    "timeWindow = 30\n",
    "\n",
    "# df = feather.read_dataframe(\"./nfb2023/202305.feather\")\n",
    "northDf = df.loc[df['RoadDirection']=='N'].reset_index(drop=True)\n",
    "vdGroups = groupVDs(northDf, each)\n",
    "groupKey = 'VD-N5-N-1.068-M-LOOP'\n",
    "\n",
    "\n",
    "\n",
    "speeds, vols, occs = [], [], []\n",
    "tmpDf = df.loc[(df['VDID'].isin(vdGroups[f\"{groupKey}\"]))].sort_values(by=['LocationMile', 'DataCollectTime'])\n",
    "\n",
    "indices = [x for x in range(0, tmpDf.shape[0]+1, tmpDf.shape[0]//each)]\n",
    "mileMatx = np.zeros((each, tmpDf.shape[0]//each))\n",
    "speedMatx = np.zeros((each, tmpDf.shape[0]//each))\n",
    "volMatx = np.zeros((each, tmpDf.shape[0]//each))\n",
    "occMatx = np.zeros((each, tmpDf.shape[0]//each))\n",
    "for i, j, k in zip(range(each), indices[:-1], indices[1:]):\n",
    "    mileMatx[i] += tmpDf.iloc[j:k,:]['LocationMile'].to_numpy()\n",
    "    speedMatx[i] += tmpDf.iloc[j:k,:]['Speed'].to_numpy()\n",
    "    volMatx[i] += tmpDf.iloc[j:k,:]['Volume'].to_numpy()\n",
    "    occMatx[i] += tmpDf.iloc[j:k,:]['Occupancy'].to_numpy()\n",
    "\n",
    "# sliceLen = int((timeWindow / 5) + 1)\n",
    "# for x in range(speedMatx.shape[1]//sliceLen*sliceLen-(sliceLen-1)):\n",
    "#     speeds.append((speedMatx[:,x:x+sliceLen][:,:-1], speedMatx[:,x:x+sliceLen][:,[-1]]))\n",
    "#     vols.append((volMatx[:,x:x+sliceLen][:,:-1], volMatx[:,x:x+sliceLen][:,[-1]]))\n",
    "#     occs.append((occMatx[:,x:x+sliceLen][:,:-1], occMatx[:,x:x+sliceLen][:,[-1]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create class `CNNDataset` inherited from `torch.utils.data.Dataset`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class CNNDataset(Dataset):\n",
    "#     def __init__(\n",
    "#             self,\n",
    "#             speed_data: list = None,\n",
    "#             volume_data: list = None,\n",
    "#             load_ckpt: bool = None,\n",
    "#             mode: str = None,\n",
    "#             ckpt_dir: str = './datasets/cnndataset'\n",
    "#     ) -> None:\n",
    "#         if (speed_data):\n",
    "#             self.speedFeature = [speed_data[x][0] for x in range(len(speed_data))]\n",
    "#             self.volFeature = [volume_data[x][0] for x in range(len(volume_data))]\n",
    "#             self.speedLabels = [speed_data[x][1][[1],:] for x in range(len(speed_data))]\n",
    "#             self.volLabels = [volume_data[x][1][[1],:] for x in range(len(volume_data))]\n",
    "        \n",
    "#         else:\n",
    "#             if (load_ckpt) and (mode == 'train'):\n",
    "#                 with h5py.File(f\"{ckpt_dir}/{mode}/{mode}_speed_feature.h5\", 'r') as file:\n",
    "#                     self.speedFeature = file[f\"{mode}_speed_feature\"][:]\n",
    "#                 with h5py.File(f\"{ckpt_dir}/{mode}/{mode}_volume_feature.h5\", 'r') as file:\n",
    "#                     self.volFeature = file[f\"{mode}_volume_feature\"][:]\n",
    "#                 with h5py.File(f\"{ckpt_dir}/{mode}/{mode}_speed_label.h5\", 'r') as file:\n",
    "#                     self.speedLabels = file[f\"{mode}_speed_label\"][:]\n",
    "#                 with h5py.File(f\"{ckpt_dir}/{mode}/{mode}_volume_label.h5\", 'r') as file:\n",
    "#                     self.volLabels = file[f\"{mode}_volume_label\"][:]\n",
    "            \n",
    "#             elif (load_ckpt) and (mode == 'test'):\n",
    "#                 with h5py.File(f\"{ckpt_dir}/{mode}/{mode}_speed_feature.h5\", 'r') as file:\n",
    "#                     self.speedFeature = file[f\"{mode}_speed_feature\"][:]\n",
    "#                 with h5py.File(f\"{ckpt_dir}/{mode}/{mode}_volume_feature.h5\", 'r') as file:\n",
    "#                     self.volFeature = file[f\"{mode}_volume_feature\"][:]\n",
    "#                 with h5py.File(f\"{ckpt_dir}/{mode}/{mode}_speed_label.h5\", 'r') as file:\n",
    "#                     self.speedLabels = file[f\"{mode}_speed_label\"][:]\n",
    "#                 with h5py.File(f\"{ckpt_dir}/{mode}/{mode}_volume_label.h5\", 'r') as file:\n",
    "#                     self.volLabels = file[f\"{mode}_volume_label\"][:]\n",
    "\n",
    "#     def __len__(self) -> int:\n",
    "#         return len(self.speedFeature)\n",
    "    \n",
    "#     def __getitem__(self, idx: int) -> torch.Tensor:\n",
    "#         f1 = torch.tensor(self.speedFeature[idx], dtype=torch.float).unsqueeze(0)\n",
    "#         f2 = torch.tensor(self.volFeature[idx], dtype=torch.float).unsqueeze(0)\n",
    "#         l1 = torch.tensor(self.speedLabels[idx], dtype=torch.float).squeeze(0)\n",
    "#         l2 = torch.tensor(self.volLabels[idx], dtype=torch.float).squeeze(0)\n",
    "#         feature = torch.cat([f1, f2])\n",
    "#         label = torch.cat([l1, l2])\n",
    "#         return feature, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create datasets\n",
    "\n",
    "First, we have to collect data from the rawdata dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EACH = 3\n",
    "speedCollection, volCollection, occCollection, laneCollection, tunnelCollection =\\\n",
    "      [], [], [], [], []\n",
    "\n",
    "# Northbound data\n",
    "northDf = df.loc[df['RoadDirection']=='N'].reset_index(drop=True)\n",
    "print(f\"northDf start grouping: {datetime.strftime(datetime.now(), '%Y-%m-%d %H:%M:%S')}\")\n",
    "northVDGrps = groupVDs(northDf, each=EACH)\n",
    "print(f\"northDf end grouping: {datetime.strftime(datetime.now(), '%Y-%m-%d %H:%M:%S')}\")\n",
    "for groupKey in northVDGrps.keys():\n",
    "    print(groupKey)\n",
    "    speeds, vols, occs, lanes, tunnels = genSamples(northDf, northVDGrps, groupKey, each=EACH, timeWindow=30)\n",
    "    speedCollection += speeds\n",
    "    volCollection += vols\n",
    "    occCollection += occs\n",
    "    laneCollection += lanes\n",
    "    tunnelCollection += tunnels\n",
    "\n",
    "# Southbound data\n",
    "southDf = df.loc[df['RoadDirection']=='S'].reset_index(drop=True)\n",
    "print(f\"southDf start grouping: {datetime.strftime(datetime.now(), '%Y-%m-%d %H:%M:%S')}\")\n",
    "southVDGrps = groupVDs(southDf, each=EACH)\n",
    "print(f\"southDf end grouping: {datetime.strftime(datetime.now(), '%Y-%m-%d %H:%M:%S')}\")\n",
    "for groupKey in southVDGrps.keys():\n",
    "    print(groupKey)\n",
    "    speeds, vols, occs, lanes, tunnels = genSamples(northDf, northVDGrps, groupKey, each=EACH, timeWindow=30)\n",
    "    speedCollection += speeds\n",
    "    volCollection += vols\n",
    "    occCollection += occs\n",
    "    laneCollection += lanes\n",
    "    tunnelCollection += tunnels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainSpeed, trainVol, trainOcc, trainNumLane, trainTunnel,\\\n",
    " testSpeed, testVol, testOcc, testNumLane, testTunnel =\\\n",
    "    train_test_split(speedCollection, volCollection, occCollection, laneCollection, tunnelCollection, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(trainSpeed), len(trainVol), len(testSpeed), len(testVol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def min_max_scaler(arr: np.ndarray, feature: str) -> np.ndarray:\n",
    "    if (feature == 'speed') or (feature == 'occ'):\n",
    "        arr = np.where(arr>=100, 100, arr)\n",
    "        return np.where(arr<0, -1, arr/100)\n",
    "    elif (feature == 'volume'):\n",
    "        arr = np.where(arr>=600, 600, arr)\n",
    "        return np.where(arr<0, -1, arr/600)\n",
    "    else:\n",
    "        raise ValueError(f\"'{feature}'\")\n",
    "\n",
    "\n",
    "class CNNDataset(Dataset):\n",
    "    def __init__(\n",
    "            self,\n",
    "            speed_data: list = None,\n",
    "            volume_data: list = None,\n",
    "            occupy_data: list = None,\n",
    "            load_ckpt: bool = None,\n",
    "            mode: str = None,\n",
    "            ckpt_dir: str = 'C:/Users/Home/PythonProjects/hwyTrafficPred/toolkits/cnndataset'\n",
    "    ) -> None:\n",
    "        if (load_ckpt):\n",
    "            with h5py.File(f\"{ckpt_dir}/{mode}/{mode}_speed_feature.h5\", 'r') as file:\n",
    "                self.speedFeature = file[f\"{mode}_speed_feature\"][:]\n",
    "            with h5py.File(f\"{ckpt_dir}/{mode}/{mode}_volume_feature.h5\", 'r') as file:\n",
    "                self.volFeature = file[f\"{mode}_volume_feature\"][:]\n",
    "            with h5py.File(f\"{ckpt_dir}/{mode}/{mode}_occupancy_feature.h5\", 'r') as file:\n",
    "                self.occFeature = file[f\"{mode}_occupancy_feature\"][:]\n",
    "            \n",
    "            with h5py.File(f\"{ckpt_dir}/{mode}/{mode}_speed_label.h5\", 'r') as file:\n",
    "                self.speedLabels = file[f\"{mode}_speed_label\"][:]\n",
    "            with h5py.File(f\"{ckpt_dir}/{mode}/{mode}_volume_label.h5\", 'r') as file:\n",
    "                self.volLabels = file[f\"{mode}_volume_label\"][:]\n",
    "            with h5py.File(f\"{ckpt_dir}/{mode}/{mode}_occupancy_label.h5\", 'r') as file:\n",
    "                self.occLabels = file[f\"{mode}_occupancy_label\"][:]\n",
    "        else:\n",
    "            self.speedFeature, self.volFeature, self.occFeature,\\\n",
    "                self.speedLabels, self.volLabels, self.occLabels = [], [], [], [], [], []\n",
    "            for x in range(len(speed_data)):\n",
    "                # Labels must be valid (>=0), or it will be dropped.\n",
    "                if (speed_data[x][1][1][0] >= 0) and (volume_data[x][1][1][0] >= 0):                \n",
    "                    self.speedFeature.append(speed_data[x][0])\n",
    "                    self.volFeature.append(volume_data[x][0])\n",
    "                    self.occFeature.append(occupy_data[x][0])\n",
    "                    self.speedLabels.append(speed_data[x][1][[1],:])\n",
    "                    self.volLabels.append(volume_data[x][1][[1],:])\n",
    "                    self.occLabels.append(occupy_data[x][1][[1],:])\n",
    "\n",
    "            with h5py.File(f\"{ckpt_dir}/{mode}/{mode}_speed_feature.h5\", 'w') as f:\n",
    "                f.create_dataset(f\"{mode}_speed_feature\", data=self.speedFeature)\n",
    "            with h5py.File(f\"{ckpt_dir}/{mode}/{mode}_volume_feature.h5\", 'w') as f:\n",
    "                f.create_dataset(f\"{mode}_volume_feature\", data=self.volFeature)\n",
    "            with h5py.File(f\"{ckpt_dir}/{mode}/{mode}_speed_label.h5\", 'w') as f:\n",
    "                f.create_dataset(f\"{mode}_speed_label\", data=self.speedLabels)\n",
    "            with h5py.File(f\"{ckpt_dir}/{mode}/{mode}_volume_label.h5\", 'w') as f:\n",
    "                f.create_dataset(f\"{mode}_volume_label\", data=self.volLabels)\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.speedFeature)\n",
    "    \n",
    "    def __getitem__(self, idx: int) -> torch.Tensor:\n",
    "        f1 = min_max_scaler(self.speedFeature[idx], 'speed')\n",
    "        f2 = min_max_scaler(self.volFeature[idx], 'volume')\n",
    "        l1 = min_max_scaler(self.speedLabels[idx], 'speed')\n",
    "        l2 = min_max_scaler(self.volLabels[idx], 'volume')\n",
    "\n",
    "        f1 = torch.tensor(f1, dtype=torch.float).unsqueeze(0)\n",
    "        f2 = torch.tensor(f2, dtype=torch.float).unsqueeze(0)\n",
    "        l1 = torch.tensor(l1, dtype=torch.float).squeeze(0)\n",
    "        l2 = torch.tensor(l2, dtype=torch.float).squeeze(0)\n",
    "        feature = torch.cat([f1, f2])\n",
    "        label = torch.cat([l1, l2])\n",
    "        return feature, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDataset = CNNDataset(speed_data=trainSpeed, volume_data=trainVol, load_ckpt=False, mode='train')\n",
    "testDataset = CNNDataset(speed_data=testSpeed, volume_data=testVol, load_ckpt=False, mode='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(trainDataset.volLabels).argmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speedCollection[0][1][[1],:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array([[100.]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDataset.speedLabels[5006179]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDataset.volLabels[5006179]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(trainDataset.speedLabels).argmin()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDataset.speedLabels[457543], trainDataset.volLabels[457543]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Directly load dataset\n",
    "\n",
    "You can also load datasets from `.h5` file if you have saved them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDataset = CNNDataset(load_ckpt=True, mode='train', ckpt_dir='./toolkits/cnndataset')\n",
    "testDataset = CNNDataset(load_ckpt=True, mode='test', ckpt_dir='./toolkits/cnndataset')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create dataloaders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Neural Network Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNRegression(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self.cnnLayer = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=2, out_channels=16, kernel_size=(2,2), stride=1, padding=0),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(1, 1, 0),\n",
    "            \n",
    "            nn.Conv2d(in_channels=16, out_channels=32, kernel_size=(2,2), stride=1, padding=0),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(1, 1, 0),\n",
    "        )\n",
    "        \n",
    "        self.fcLayer = nn.Sequential(\n",
    "            nn.Linear(32 * 1 * 4, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 2),\n",
    "        )\n",
    "\n",
    "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        self.to(self.device)\n",
    "\n",
    "    def forward(self, x) -> torch.Tensor:\n",
    "        x = self.cnnLayer(x)\n",
    "        x = x.flatten(1)\n",
    "        x = self.fcLayer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparams for training\n",
    "batch_size = 256\n",
    "lr = 1e-3\n",
    "n_epochs = 200\n",
    "\n",
    "# Prepare datasets and dataloaders\n",
    "trainDataset, testDataset = load_next_5min()\n",
    "trainLoader = DataLoader(trainDataset, batch_size=batch_size, shuffle=True)\n",
    "testLoader = DataLoader(testDataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "model = CNNRegression()\n",
    "optimizer = optim.Adam(params=model.parameters(), lr=lr, weight_decay=1e-8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(n_epochs):\n",
    "    # Switch to train mode\n",
    "    model.train()\n",
    "    \n",
    "    # Record Info in training\n",
    "    train_loss = []\n",
    "\n",
    "    for batch in tqdm(trainLoader):\n",
    "        X, y = batch\n",
    "        logits = model(X.to(model.device))\n",
    "        loss = F.mse_loss(logits, y.to(model.device))\n",
    "        \n",
    "        # Compute gradients and update model params\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss.append(loss.item())\n",
    "\n",
    "    # Compute the average train_loss\n",
    "    train_loss = sum(train_loss) / len(train_loss)\n",
    "    print(f\"[ Train | {epoch + 1:d}/{n_epochs:d} ] loss = {train_loss:.5f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "route-plan",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
