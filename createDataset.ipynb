{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine\n",
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import feather\n",
    "import os\n",
    "import h5py\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "# connect to db\n",
    "user = 'root'\n",
    "pswd = 'Curry5566'\n",
    "host = '127.0.0.1'\n",
    "port = '3306'\n",
    "db = 'transport'\n",
    "engine = create_engine(f\"mysql+pymysql://{user}:{pswd}@{host}:{port}/{db}?charset=utf8\")\n",
    "\n",
    "\n",
    "def getEndDate(startDate: str, days: int) -> str:\n",
    "    startDate += ' 00:00:00'\n",
    "    end = str((datetime.strptime(startDate, '%Y-%m-%d %H:%M:%S') + timedelta(days=days)).replace(microsecond=0))\n",
    "    return end\n",
    "\n",
    "def getRollingMean(startDate: str, endDate: str) -> pd.DataFrame:\n",
    "    \"\"\" Get the rolling mean from db\n",
    "        ```text\n",
    "        ---\n",
    "        @Params\n",
    "        startDate: The date for start, format='%Y-%m-%d'\n",
    "        endDate: The date for end, format='%Y-%m-%d'\n",
    "\n",
    "        ---\n",
    "        @Returns\n",
    "        DataFrame\n",
    "        ```\n",
    "    \"\"\"\n",
    "    sql  = \" SELECT \"\n",
    "    sql += \" \tSTAC.VDID, STAC.RoadName, STAC.`Start`, STAC.`End`, \"\n",
    "    sql += \" \tSTAC.RoadDirection, DYMC.Speed, DYMC.Occupancy, DYMC.Volume, \"\n",
    "    sql += \" \tSTAC.LocationMile, DYMC.DataCollectTime \"\n",
    "    sql += \" FROM ( \"\n",
    "    sql += \" \tSELECT \"\n",
    "    sql += \" \t\tVDSTC.id, VDSTC.VDID, ROAD.RoadName, SEC.`Start`, SEC.`End`, \"\n",
    "    sql += \" \t\tVDSTC.RoadDirection, VDSTC.LocationMile \"\n",
    "    sql += \" \tFROM vd_static_n5 VDSTC \"\n",
    "    sql += \" \tJOIN road_info ROAD ON VDSTC.RoadInfoID = ROAD.id \"\n",
    "    sql += \" \tJOIN section_info SEC ON ROAD.id = SEC.RoadInfoID \"\n",
    "    sql += \" \tAND VDSTC.LocationMile >= SEC.StartKM \"\n",
    "    sql += \" \tAND VDSTC.LocationMile <= SEC.EndKM \"\n",
    "    sql += \" \tWHERE VDSTC.Mainlane = 1 \"\n",
    "    sql += \" ) STAC JOIN ( \"\n",
    "    sql += \" \tSELECT \"\n",
    "    sql += \" \t\tVdStaticID, \"\n",
    "    sql += \" \t\tCASE \"\n",
    "    sql += \" \t\t\tWHEN MIN(Speed) = -99 THEN -99 \"\n",
    "    sql += \" \t\t\tELSE AVG(Speed) \"\n",
    "    sql += \" \t\tEND AS Speed,  \"\n",
    "    sql += \" \t\tCASE \"\n",
    "    sql += \" \t\t\tWHEN MIN(Occupancy) = -99 THEN -99 \"\n",
    "    sql += \" \t\t\tELSE AVG(Occupancy) \"\n",
    "    sql += \" \t\tEND AS Occupancy,  \"\n",
    "    sql += \" \t\tCASE \"\n",
    "    sql += \" \t\t\tWHEN MIN(Volume) = -99 THEN -99 \"\n",
    "    sql += \" \t\t\tELSE AVG(Volume) \"\n",
    "    sql += \" \t\tEND AS Volume, \"\n",
    "    sql += \" \t\tMAX(DataCollectTime) AS DataCollectTime, \"\n",
    "    sql += \" \t\t(UNIX_TIMESTAMP(DataCollectTime)-UNIX_TIMESTAMP(%(start)s)) DIV 300 \"\n",
    "    sql += \" \tFROM vd_dynamic_detail_n5_202301 \"\n",
    "    sql += \" \tWHERE id BETWEEN ( \"\n",
    "    sql += \" \t\tSELECT id FROM vd_dynamic_detail_n5_202301 \"\n",
    "    sql += \" \t\tWHERE DataCollectTime = %(start)s \"\n",
    "    sql += \" \t\tORDER BY id LIMIT 1 \"\n",
    "    sql += \" \t) AND ( \"\n",
    "    sql += \" \t\tSELECT id FROM vd_dynamic_detail_n5_202301 \"\n",
    "    sql += \" \t\tWHERE DataCollectTime < %(end)s \"\n",
    "    sql += \" \t\tORDER BY id DESC LIMIT 1 \"\n",
    "    sql += \" \t) \"\n",
    "    sql += \" \tGROUP BY VdStaticID, (UNIX_TIMESTAMP(DataCollectTime)-UNIX_TIMESTAMP(%(start)s)) DIV 300 \"\n",
    "    sql += \" ) DYMC ON STAC.id = DYMC.VdStaticID \"\n",
    "    sql += \" ORDER BY STAC.RoadDirection, STAC.LocationMile, DYMC.DataCollectTime; \"\n",
    "\n",
    "    df = pd.read_sql(sql, con=engine, params={'start': startDate, 'end': endDate})\n",
    "    engine.dispose()\n",
    "    return df.sort_values(by=['RoadDirection','DataCollectTime','LocationMile']).reset_index(drop=True)\n",
    "\n",
    "def getRollingMeanDaily(selectDate: str) -> pd.DataFrame:\n",
    "    sql  = \" SELECT \"\n",
    "    sql += \" \tSTAC.VDID, STAC.RoadName, STAC.`Start`, STAC.`End`, \"\n",
    "    sql += \" \tSTAC.RoadDirection, DYMC.Speed, DYMC.Occupancy, DYMC.Volume, \"\n",
    "    sql += \" \tSTAC.ActualLaneNum, STAC.LocationMile, STAC.isTunnel, DYMC.DataCollectTime \"\n",
    "    sql += \" FROM ( \"\n",
    "    sql += \" \tSELECT \"\n",
    "    sql += \" \t\tVDSTC.id, VDSTC.VDID, ROAD.RoadName, SEC.`Start`, SEC.`End`, \"\n",
    "    sql += \" \t\tVDSTC.ActualLaneNum, VDSTC.RoadDirection, VDSTC.LocationMile, \"\n",
    "    sql += \"        CASE \"\n",
    "    sql += \" \t        WHEN VDSTC.RoadDirection = 'S' AND VDSTC.LocationMile BETWEEN 15.203 AND 28.128 THEN 1 \"\n",
    "    sql += \" \t        WHEN VDSTC.RoadDirection = 'N' AND VDSTC.LocationMile BETWEEN 15.179 AND 28.134 THEN 1 \"\n",
    "    sql += \" \t        ELSE 0 \"\n",
    "    sql += \"        END AS isTunnel \"\n",
    "    sql += \" \tFROM fwy_n5.vd_static_2023 VDSTC \"\n",
    "    sql += \" \tJOIN transport.road_info ROAD ON VDSTC.RoadInfoID = ROAD.id \"\n",
    "    sql += \" \tJOIN transport.section_info SEC ON ROAD.id = SEC.RoadInfoID \"\n",
    "    sql += \" \tAND VDSTC.LocationMile >= SEC.StartKM \"\n",
    "    sql += \" \tAND VDSTC.LocationMile <= SEC.EndKM \"\n",
    "    sql += \" \tWHERE VDSTC.Mainlane = 1 \"\n",
    "    sql += \" ) STAC JOIN ( \"\n",
    "    sql += \" \tSELECT \"\n",
    "    sql += \" \t\tVdStaticID, \"\n",
    "    sql += \" \t\tCASE \"\n",
    "    sql += \" \t\t\tWHEN MIN(Speed) = -99 THEN -99 \"\n",
    "    sql += \" \t\t\tELSE AVG(Speed) \"\n",
    "    sql += \" \t\tEND AS Speed,  \"\n",
    "    sql += \" \t\tCASE \"\n",
    "    sql += \" \t\t\tWHEN MIN(Occupancy) = -99 THEN -99 \"\n",
    "    sql += \" \t\t\tELSE AVG(Occupancy) \"\n",
    "    sql += \" \t\tEND AS Occupancy,  \"\n",
    "    sql += \" \t\tCASE \"\n",
    "    sql += \" \t\t\tWHEN MIN(Volume) = -99 THEN -99 \"\n",
    "    sql += \" \t\t\tELSE AVG(Volume) \"\n",
    "    sql += \" \t\tEND AS Volume, \"\n",
    "    sql += \" \t\tMAX(DataCollectTime) AS DataCollectTime, \"\n",
    "    sql += \" \t\t(UNIX_TIMESTAMP(DataCollectTime)-UNIX_TIMESTAMP(%(selectDate)s)) DIV 300 \"\n",
    "    sql += \" \tFROM fwy_n5.vd_dynamic_detail_{} \".format(selectDate.replace('-',''))\n",
    "    sql += \" \tGROUP BY VdStaticID, (UNIX_TIMESTAMP(DataCollectTime)-UNIX_TIMESTAMP(%(selectDate)s)) DIV 300 \"\n",
    "    sql += \" ) DYMC ON STAC.id = DYMC.VdStaticID \"\n",
    "    sql += \" ORDER BY STAC.RoadDirection, STAC.LocationMile, DYMC.DataCollectTime; \"\n",
    "\n",
    "    df = pd.read_sql(sql, con=engine, params={'selectDate': selectDate})\n",
    "    engine.dispose()\n",
    "    return df.sort_values(by=['RoadDirection','DataCollectTime','LocationMile']).reset_index(drop=True)\n",
    "\n",
    "def groupVDs(df: pd.DataFrame, each: int) -> dict:\n",
    "    \"\"\" Get the dict of VD groups\n",
    "        ```text\n",
    "        ---\n",
    "        @Params\n",
    "        df: DataFrame which is referenced by.\n",
    "        each: The quantity of VDs would be considered as a group.\n",
    "\n",
    "        ---\n",
    "        @Returns\n",
    "        vdGroups: The keys are the VDs we focus on, and the values are the collections of VDs which are correlated corresponding to the keys.\n",
    "        ```\n",
    "    \"\"\"\n",
    "    vdGroups = {}\n",
    "    lb = each // 2\n",
    "    ub = each - (each // 2)\n",
    "    for vdid in df['VDID'].unique():\n",
    "        vdGroups.setdefault(f\"{vdid}\", [])\n",
    "    for no, vdid in enumerate(df['VDID'].unique()):\n",
    "        startIdx = max(no-lb, 0)\n",
    "        endIdx = min(no+ub, len(df['VDID'].unique())-1)\n",
    "        vdGroups[f\"{vdid}\"] += list(df['VDID'].unique()[startIdx:no]) + list(df['VDID'].unique()[no:endIdx])\n",
    "\n",
    "    delList = []\n",
    "    for k in vdGroups.keys():\n",
    "        if (len(vdGroups[k]) != each):\n",
    "            delList.append(k)\n",
    "    for k in delList:\n",
    "        del vdGroups[k]\n",
    "    \n",
    "    return vdGroups\n",
    "\n",
    "def genSamples(df: pd.DataFrame, vdGroups: dict, groupKey: str, each: int, timeWindow: int = 30) -> tuple:\n",
    "    \"\"\" Generate samples for each traffic data (speed, volume, and occupancy)\n",
    "        ```text\n",
    "        ---\n",
    "        @Params\n",
    "        df: \n",
    "        vdGroups: The outpur of groupVDs(),\n",
    "        groupKey: The key of vdGroups,\n",
    "        each: The quantity of VDs would be considered as a group,\n",
    "        timeWindow: The length of period we consider, and the default value is 30 (minutes).\n",
    "\n",
    "        ---\n",
    "        @Returns\n",
    "        speeds: list with each item as a tuple, all of them are represented (X,y).\n",
    "        vols: list with each item as a tuple, all of them are represented (X,y).\n",
    "        occs: list with each item as a tuple, all of them are represented (X,y).\n",
    "        ```\n",
    "    \"\"\"\n",
    "    speeds, vols, occs = [], [], []\n",
    "    tmpDf = df.loc[(df['VDID'].isin(vdGroups[f\"{groupKey}\"]))].sort_values(by=['LocationMile', 'DataCollectTime'])\n",
    "\n",
    "    indices = [x for x in range(0, tmpDf.shape[0]+1, tmpDf.shape[0]//each)]\n",
    "    speedMatx = np.zeros((each, tmpDf.shape[0]//each))\n",
    "    volMatx = np.zeros((each, tmpDf.shape[0]//each))\n",
    "    occMatx = np.zeros((each, tmpDf.shape[0]//each))\n",
    "    for i, j, k in zip(range(each), indices[:-1], indices[1:]):\n",
    "        speedMatx[i] += tmpDf.iloc[j:k,:]['Speed'].to_numpy()\n",
    "        volMatx[i] += tmpDf.iloc[j:k,:]['Volume'].to_numpy()\n",
    "        occMatx[i] += tmpDf.iloc[j:k,:]['Occupancy'].to_numpy()\n",
    "\n",
    "    sliceLen = int((timeWindow / 5) + 1)\n",
    "    for x in range(speedMatx.shape[1]//sliceLen*sliceLen-(sliceLen-1)):\n",
    "        speeds.append((speedMatx[:,x:x+sliceLen][:,:-1], speedMatx[:,x:x+sliceLen][:,[-1]]))\n",
    "        vols.append((volMatx[:,x:x+sliceLen][:,:-1], volMatx[:,x:x+sliceLen][:,[-1]]))\n",
    "        occs.append((occMatx[:,x:x+sliceLen][:,:-1], occMatx[:,x:x+sliceLen][:,[-1]]))\n",
    "    \n",
    "    return speeds, vols, occs\n",
    "\n",
    "def genTensors(speeds: list, vols: list) -> list:\n",
    "    \"\"\" Generate torch.Tensors.\n",
    "        The sizes of the tensors are `[batch, 2, each, 6]`, and `each` depends on how many VDs regarded as a group.\n",
    "    \"\"\"\n",
    "    dataCollection = []\n",
    "    for s, v in zip(speeds, vols):\n",
    "        s = torch.tensor(s, dtype=torch.float).unsqueeze(0).unsqueeze(0)\n",
    "        v = torch.tensor(v, dtype=torch.float).unsqueeze(0).unsqueeze(0)\n",
    "        dataCollection.append(torch.concat([s, v], dim=1))\n",
    "    return dataCollection\n",
    "\n",
    "def train_test_split(speedCollection, volCollection, train_size=None, test_size=None, random_number=42):\n",
    "    np.random.seed(random_number)\n",
    "    if train_size:\n",
    "        trainDataIdx = np.random.choice(\n",
    "            len(speedCollection),\n",
    "            int(train_size * len(speedCollection)),\n",
    "            replace=False\n",
    "        )\n",
    "        testDataIdx = set([i for i in range(len(speedCollection))]) -\\\n",
    "                      set(trainDataIdx)\n",
    "    \n",
    "    elif test_size:\n",
    "        testDataIdx = np.random.choice(\n",
    "            len(speedCollection),\n",
    "            int(test_size * len(speedCollection)),\n",
    "            replace=False\n",
    "        )\n",
    "        trainDataIdx = set([i for i in range(len(speedCollection))]) -\\\n",
    "                       set(testDataIdx)\n",
    "        \n",
    "    trainSpeed = list(pd.Series(speedCollection)[list(trainDataIdx)])\n",
    "    trainVol = list(pd.Series(volCollection)[list(trainDataIdx)])\n",
    "    testSpeed = list(pd.Series(speedCollection)[list(testDataIdx)])\n",
    "    testVol = list(pd.Series(volCollection)[list(testDataIdx)])\n",
    "\n",
    "    return trainSpeed, trainVol, testSpeed, testVol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 取得一年份資料\n",
    "# firstDate = list(map(lambda x: datetime.strftime(x, '%Y-%m-%d'), list(pd.date_range('2023-01-01', '2023-12-31', freq='MS'))))\n",
    "# lastDate = list(map(lambda x: datetime.strftime(x, '%Y-%m-%d'), list(pd.date_range('2023-01-01', '2023-12-31', freq='ME'))))\n",
    "# for first, last in zip(firstDate, lastDate):\n",
    "#     dataframes = []\n",
    "#     dateList = list(map(lambda x: datetime.strftime(x, '%Y-%m-%d'), list(pd.date_range(first, last))))\n",
    "#     for date in dateList:\n",
    "#         print(date)\n",
    "#         dataframes.append(getRollingMeanDaily(date))\n",
    "#     dataframes = pd.concat(dataframes).reset_index(drop=True)\n",
    "#     display(dataframes)\n",
    "#     feather.write_dataframe(dataframes, dest=f\"./nfb2023/{date[:7].replace('-','')}.feather\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "monthlyStarts = list(map(lambda x: datetime.strftime(x, '%Y-%m-%d'), list(pd.date_range('2023-02-01', '2023-02-28', freq='MS'))))\n",
    "monthlyEnds = list(map(lambda x: datetime.strftime(x, '%Y-%m-%d'), list(pd.date_range('2023-02-01', '2023-02-28', freq='ME'))))\n",
    "\n",
    "for start, end in zip(monthlyStarts, monthlyEnds):\n",
    "    dataframes = []\n",
    "    dateList = list(map(lambda x: datetime.strftime(x, '%Y-%m-%d'), list(pd.date_range(start, end))))\n",
    "    print(start[:7].replace('-',''))\n",
    "    for date in dateList:\n",
    "        print(date)\n",
    "        dataframes.append(getRollingMeanDaily(date))\n",
    "    dataframes = pd.concat(dataframes).reset_index(drop=True)\n",
    "    # display(dataframes)\n",
    "    feather.write_dataframe(dataframes, dest=f\"./nfb2023/{start[:7].replace('-','')}.feather\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.listdir('./nfb2023')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = []\n",
    "for filename in os.listdir('./nfb2023'):\n",
    "    monthlyDf = feather.read_dataframe(f\"./nfb2023/{filename}\")\n",
    "    if (len(df) == 0):\n",
    "        df.append(monthlyDf)\n",
    "    else:\n",
    "        currDf = pd.concat(df).reset_index(drop=True)\n",
    "        monthlyDf = monthlyDf.loc[monthlyDf['VDID'].isin(set(currDf['VDID']))]\n",
    "        df.append(monthlyDf)\n",
    "df = pd.concat(df).reset_index(drop=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('./df.csv', index=False, encoding='utf_8_sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # TODO: main\n",
    "# if __name__ == '__main__':\n",
    "#     # # read feather files to get dataframes\n",
    "#     # startDate = '2023-01-01'\n",
    "#     # endDate = getEndDate(startDate, days=10)\n",
    "#     # # df = getRollingMean(startDate, endDate)\n",
    "#     # df = feather.read_dataframe('./20230101-20230110.feather').sort_values(by=['RoadDirection','DataCollectTime','LocationMile']).reset_index(drop=True)\n",
    "    \n",
    "#     # Northbound data\n",
    "#     northDf = df.loc[df['RoadDirection']=='N'].reset_index(drop=True)\n",
    "#     each = 3\n",
    "#     vdGroups = groupVDs(northDf, each)    \n",
    "#     speedDataset, volDataset, occDataset = [], [], []\n",
    "#     for groupKey in vdGroups.keys():\n",
    "#         speeds, vols, occs = genSamples(northDf, vdGroups, groupKey, each, timeWindow=30)\n",
    "#         speedDataset.append(speeds)\n",
    "#         volDataset.append(vols)\n",
    "#         occDataset.append(occs)\n",
    "\n",
    "#     # Southbound data\n",
    "#     southDf = df.loc[df['RoadDirection']=='S'].reset_index(drop=True)\n",
    "#     each = 3\n",
    "#     vdGroups = groupVDs(southDf, each)    \n",
    "#     speedDataset, volDataset, occDataset = [], [], []\n",
    "#     for groupKey in vdGroups.keys():\n",
    "#         speeds, vols, occs = genSamples(southDf, vdGroups, groupKey, each, timeWindow=30)\n",
    "#         speedDataset.append(speeds)\n",
    "#         volDataset.append(vols)\n",
    "#         occDataset.append(occs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## test cell for missing data\n",
    "# This part is genSamples()\n",
    "each = 3\n",
    "timeWindow = 30\n",
    "\n",
    "# df = feather.read_dataframe(\"./nfb2023/202305.feather\")\n",
    "northDf = df.loc[df['RoadDirection']=='N'].reset_index(drop=True)\n",
    "vdGroups = groupVDs(northDf, each)\n",
    "groupKey = 'VD-N5-N-1.068-M-LOOP'\n",
    "\n",
    "\n",
    "\n",
    "speeds, vols, occs = [], [], []\n",
    "tmpDf = df.loc[(df['VDID'].isin(vdGroups[f\"{groupKey}\"]))].sort_values(by=['LocationMile', 'DataCollectTime'])\n",
    "\n",
    "indices = [x for x in range(0, tmpDf.shape[0]+1, tmpDf.shape[0]//each)]\n",
    "mileMatx = np.zeros((each, tmpDf.shape[0]//each))\n",
    "speedMatx = np.zeros((each, tmpDf.shape[0]//each))\n",
    "volMatx = np.zeros((each, tmpDf.shape[0]//each))\n",
    "occMatx = np.zeros((each, tmpDf.shape[0]//each))\n",
    "for i, j, k in zip(range(each), indices[:-1], indices[1:]):\n",
    "    mileMatx[i] += tmpDf.iloc[j:k,:]['LocationMile'].to_numpy()\n",
    "    speedMatx[i] += tmpDf.iloc[j:k,:]['Speed'].to_numpy()\n",
    "    volMatx[i] += tmpDf.iloc[j:k,:]['Volume'].to_numpy()\n",
    "    occMatx[i] += tmpDf.iloc[j:k,:]['Occupancy'].to_numpy()\n",
    "\n",
    "# sliceLen = int((timeWindow / 5) + 1)\n",
    "# for x in range(speedMatx.shape[1]//sliceLen*sliceLen-(sliceLen-1)):\n",
    "#     speeds.append((speedMatx[:,x:x+sliceLen][:,:-1], speedMatx[:,x:x+sliceLen][:,[-1]]))\n",
    "#     vols.append((volMatx[:,x:x+sliceLen][:,:-1], volMatx[:,x:x+sliceLen][:,[-1]]))\n",
    "#     occs.append((occMatx[:,x:x+sliceLen][:,:-1], occMatx[:,x:x+sliceLen][:,[-1]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create class `CNNDataset` inherited from `torch.utils.data.Dataset`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNDataset(Dataset):\n",
    "    def __init__(\n",
    "            self,\n",
    "            speed_data: list = None,\n",
    "            volume_data: list = None,\n",
    "            load_ckpt: bool = None,\n",
    "            mode: str = None,\n",
    "            ckpt_dir: str = './datasets/cnndataset'\n",
    "    ) -> None:\n",
    "        if (speed_data):\n",
    "            self.speedFeature = [speed_data[x][0] for x in range(len(speed_data))]\n",
    "            self.volFeature = [volume_data[x][0] for x in range(len(volume_data))]\n",
    "            self.speedLabels = [speed_data[x][1][[1],:] for x in range(len(speed_data))]\n",
    "            self.volLabels = [volume_data[x][1][[1],:] for x in range(len(volume_data))]\n",
    "        \n",
    "        else:\n",
    "            if (load_ckpt) and (mode == 'train'):\n",
    "                with h5py.File(f\"{ckpt_dir}/{mode}/{mode}_speed_feature.h5\", 'r') as file:\n",
    "                    self.speedFeature = file[f\"{mode}_speed_feature\"][:]\n",
    "                with h5py.File(f\"{ckpt_dir}/{mode}/{mode}_volume_feature.h5\", 'r') as file:\n",
    "                    self.volFeature = file[f\"{mode}_volume_feature\"][:]\n",
    "                with h5py.File(f\"{ckpt_dir}/{mode}/{mode}_speed_label.h5\", 'r') as file:\n",
    "                    self.speedLabels = file[f\"{mode}_speed_label\"][:]\n",
    "                with h5py.File(f\"{ckpt_dir}/{mode}/{mode}_volume_label.h5\", 'r') as file:\n",
    "                    self.volLabels = file[f\"{mode}_volume_label\"][:]\n",
    "            \n",
    "            elif (load_ckpt) and (mode == 'test'):\n",
    "                with h5py.File(f\"{ckpt_dir}/{mode}/{mode}_speed_feature.h5\", 'r') as file:\n",
    "                    self.speedFeature = file[f\"{mode}_speed_feature\"][:]\n",
    "                with h5py.File(f\"{ckpt_dir}/{mode}/{mode}_volume_feature.h5\", 'r') as file:\n",
    "                    self.volFeature = file[f\"{mode}_volume_feature\"][:]\n",
    "                with h5py.File(f\"{ckpt_dir}/{mode}/{mode}_speed_label.h5\", 'r') as file:\n",
    "                    self.speedLabels = file[f\"{mode}_speed_label\"][:]\n",
    "                with h5py.File(f\"{ckpt_dir}/{mode}/{mode}_volume_label.h5\", 'r') as file:\n",
    "                    self.volLabels = file[f\"{mode}_volume_label\"][:]\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.speedFeature)\n",
    "    \n",
    "    def __getitem__(self, idx: int) -> torch.Tensor:\n",
    "        f1 = torch.tensor(self.speedFeature[idx], dtype=torch.float).unsqueeze(0)\n",
    "        f2 = torch.tensor(self.volFeature[idx], dtype=torch.float).unsqueeze(0)\n",
    "        l1 = torch.tensor(self.speedLabels[idx], dtype=torch.float)\n",
    "        l2 = torch.tensor(self.volLabels[idx], dtype=torch.float)\n",
    "        feature = torch.cat([f1, f2])\n",
    "        label = torch.cat([l1, l2])\n",
    "        return feature, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create datasets\n",
    "\n",
    "First, we have to collect data from the rawdata dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EACH = 3\n",
    "speedCollection, volCollection, occCollection = [], [], []\n",
    "\n",
    "# Northbound data\n",
    "northDf = df.loc[df['RoadDirection']=='N'].reset_index(drop=True)\n",
    "print(f\"northDf start grouping: {datetime.strftime(datetime.now(), '%Y-%m-%d %H:%M:%S')}\")\n",
    "northVDGrps = groupVDs(northDf, each=EACH)\n",
    "print(f\"northDf end grouping: {datetime.strftime(datetime.now(), '%Y-%m-%d %H:%M:%S')}\")\n",
    "for groupKey in northVDGrps.keys():\n",
    "    print(groupKey)\n",
    "    speeds, vols, occs = genSamples(northDf, northVDGrps, groupKey, each=EACH, timeWindow=30)\n",
    "    speedCollection += speeds\n",
    "    volCollection += vols\n",
    "    occCollection += occs\n",
    "\n",
    "# Southbound data\n",
    "southDf = df.loc[df['RoadDirection']=='S'].reset_index(drop=True)\n",
    "print(f\"southDf start grouping: {datetime.strftime(datetime.now(), '%Y-%m-%d %H:%M:%S')}\")\n",
    "southVDGrps = groupVDs(southDf, each=EACH)\n",
    "print(f\"southDf end grouping: {datetime.strftime(datetime.now(), '%Y-%m-%d %H:%M:%S')}\")\n",
    "# speedDataset, volDataset, occDataset = [], [], []\n",
    "for groupKey in southVDGrps.keys():\n",
    "    print(groupKey)\n",
    "    speeds, vols, occs = genSamples(southDf, southVDGrps, groupKey, each=EACH, timeWindow=30)\n",
    "    speedCollection += speeds\n",
    "    volCollection += vols\n",
    "    occCollection += occs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainSpeed, trainVol, testSpeed, testVol =\\\n",
    "    train_test_split(speedCollection, volCollection, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDataset = CNNDataset(speed_data=trainSpeed, volume_data=trainVol)\n",
    "testDataset = CNNDataset(speed_data=testSpeed, volume_data=testVol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDataset.speedFeature[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the dataset as `.h5`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File('./datasets/cnndataset/train/train_speed_feature.h5', 'w') as f:\n",
    "    f.create_dataset('train_speed_feature', data=trainDataset.speedFeature)\n",
    "\n",
    "with h5py.File('./datasets/cnndataset/train/train_volume_feature.h5', 'w') as f:\n",
    "    f.create_dataset('train_volume_feature', data=trainDataset.volFeature)\n",
    "\n",
    "with h5py.File('./datasets/cnndataset/train/train_speed_label.h5', 'w') as f:\n",
    "    f.create_dataset('train_speed_label', data=trainDataset.speedLabels)\n",
    "\n",
    "with h5py.File('./datasets/cnndataset/train/train_volume_label.h5', 'w') as f:\n",
    "    f.create_dataset('train_volume_label', data=trainDataset.volLabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File('./datasets/cnndataset/test/test_speed_feature.h5', 'w') as f:\n",
    "    f.create_dataset('test_speed_feature', data=testDataset.speedFeature)\n",
    "\n",
    "with h5py.File('./datasets/cnndataset/test/test_volume_feature.h5', 'w') as f:\n",
    "    f.create_dataset('test_volume_feature', data=testDataset.volFeature)\n",
    "\n",
    "with h5py.File('./datasets/cnndataset/test/test_speed_label.h5', 'w') as f:\n",
    "    f.create_dataset('test_speed_label', data=testDataset.speedLabels)\n",
    "\n",
    "with h5py.File('./datasets/cnndataset/test/test_volume_label.h5', 'w') as f:\n",
    "    f.create_dataset('test_volume_label', data=testDataset.volLabels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Directly load dataset\n",
    "\n",
    "You can also load datasets from `.h5` file if you have saved them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDataset = CNNDataset(load_ckpt=True, mode='train', ckpt_dir='./datasets/cnndataset')\n",
    "testDataset = CNNDataset(load_ckpt=True, mode='test', ckpt_dir='./datasets/cnndataset')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 256\n",
    "\n",
    "trainLoader = DataLoader(trainDataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "testLoader = DataLoader(testDataset, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Neural Network Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNModel(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self.cnnLayer = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=2, out_channels=16, kernel_size=(1,1), stride=1, padding=0),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=16, out_channels=32, kernel_size=(1,1), stride=1, padding=0),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        # self.fcLayer = nn.Sequential(\n",
    "        #     nn.Linear(),\n",
    "        # )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.cnnLayer(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CNNModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, data in enumerate(trainLoader):\n",
    "    X, y = data\n",
    "    model(X)\n",
    "    break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "route-plan",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
