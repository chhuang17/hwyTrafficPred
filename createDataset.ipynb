{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Home\\anaconda3\\envs\\route-plan\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from sqlalchemy import create_engine\n",
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from dotenv import load_dotenv\n",
    "from toolkits.datapreparing import download_monthly_tables, collect_data\n",
    "from toolkits.datasets import CNNDataset, train_test_split, load_next_5min\n",
    "from tqdm.auto import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import feather\n",
    "import os\n",
    "import h5py\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## main\n",
    "\n",
    "# download_monthly_tables(start='2023-01-01', end='2023-12-31', dest_dir='./nfb2023', file_format='feather')\n",
    "speedCollection, volCollection, occCollection, laneCollection, tunnelCollection = collect_data()\n",
    "\n",
    "trainSpeed, trainVol, trainOcc, trainNumLane, trainTunnel,\\\n",
    "testSpeed, testVol, testOcc, testNumLane, testTunnel =\\\n",
    "    train_test_split(speedCollection, volCollection, occCollection, laneCollection, tunnelCollection, test_size=0.2)\n",
    "\n",
    "trainDataset = CNNDataset(speed_data=trainSpeed, volume_data=trainVol, occupy_data=trainOcc,\n",
    "                          lane_data=trainNumLane, tunnel_data=trainTunnel, load_ckpt=False, mode='train')\n",
    "testDataset = CNNDataset(speed_data=testSpeed, volume_data=testVol, occupy_data=testOcc,\n",
    "                         lane_data=testNumLane, tunnel_data=testTunnel, load_ckpt=False, mode='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.7050, 0.8420, 0.7810, 0.8230, 0.6750, 0.8050],\n",
       "         [0.8010, 0.7880, 0.8160, 0.8040, 0.7190, 0.6900],\n",
       "         [0.8680, 0.8480, 0.8790, 0.8620, 0.7550, 0.6620]],\n",
       "\n",
       "        [[0.0062, 0.0055, 0.0073, 0.0067, 0.0047, 0.0077],\n",
       "         [0.0060, 0.0055, 0.0070, 0.0068, 0.0047, 0.0073],\n",
       "         [0.0065, 0.0070, 0.0058, 0.0062, 0.0048, 0.0082]],\n",
       "\n",
       "        [[0.0270, 0.0270, 0.0370, 0.0320, 0.0230, 0.0370],\n",
       "         [0.0280, 0.0260, 0.0320, 0.0330, 0.0200, 0.0360],\n",
       "         [0.0280, 0.0330, 0.0250, 0.0280, 0.0220, 0.0400]],\n",
       "\n",
       "        [[0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000],\n",
       "         [0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000],\n",
       "         [0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000]],\n",
       "\n",
       "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000]]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainDataset[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Directly load dataset\n",
    "\n",
    "You can also load datasets from `.h5` file if you have saved them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDataset = CNNDataset(load_ckpt=True, mode='train', ckpt_dir='./toolkits/cnndataset')\n",
    "testDataset = CNNDataset(load_ckpt=True, mode='test', ckpt_dir='./toolkits/cnndataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDataset.tunnelFeature[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create dataloaders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Neural Network Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNRegression(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self.cnnLayer = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=2, out_channels=16, kernel_size=(2,2), stride=1, padding=0),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(1, 1, 0),\n",
    "            \n",
    "            nn.Conv2d(in_channels=16, out_channels=32, kernel_size=(2,2), stride=1, padding=0),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(1, 1, 0),\n",
    "        )\n",
    "        \n",
    "        self.fcLayer = nn.Sequential(\n",
    "            nn.Linear(32 * 1 * 4, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 2),\n",
    "        )\n",
    "\n",
    "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        self.to(self.device)\n",
    "\n",
    "    def forward(self, x) -> torch.Tensor:\n",
    "        x = self.cnnLayer(x)\n",
    "        x = x.flatten(1)\n",
    "        x = self.fcLayer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparams for training\n",
    "batch_size = 256\n",
    "lr = 1e-3\n",
    "n_epochs = 200\n",
    "\n",
    "# Prepare datasets and dataloaders\n",
    "trainDataset, testDataset = load_next_5min()\n",
    "trainLoader = DataLoader(trainDataset, batch_size=batch_size, shuffle=True)\n",
    "testLoader = DataLoader(testDataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "model = CNNRegression()\n",
    "optimizer = optim.Adam(params=model.parameters(), lr=lr, weight_decay=1e-8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(n_epochs):\n",
    "    # Switch to train mode\n",
    "    model.train()\n",
    "    \n",
    "    # Record Info in training\n",
    "    train_loss = []\n",
    "\n",
    "    for batch in tqdm(trainLoader):\n",
    "        X, y = batch\n",
    "        logits = model(X.to(model.device))\n",
    "        loss = F.mse_loss(logits, y.to(model.device))\n",
    "        \n",
    "        # Compute gradients and update model params\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss.append(loss.item())\n",
    "\n",
    "    # Compute the average train_loss\n",
    "    train_loss = sum(train_loss) / len(train_loss)\n",
    "    print(f\"[ Train | {epoch + 1:d}/{n_epochs:d} ] loss = {train_loss:.5f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "route-plan",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
